{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly CapableMultimodal Models\nGemini Team, Google1\nThis report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilitiesacross image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nanosizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constraineduse-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra modeladvances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achievehuman-expert performance on the well-studied exam benchmark MMLU, and improving the state of theart in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities ofthe Gemini family in cross-modal reasoning and language understanding will enable a wide variety ofuse cases. We discuss our approach toward post-training and deploying Gemini models responsibly tousers through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.\n1. Introduction\nWe present Gemini, a family of highly capable multimodal models developed at Google. We trainedGemini models jointly across image, audio, video, and text data for the purpose of building a modelwith both strong generalist capabilities across modalities alongside cutting-edge understanding andreasoning performance in each respective domain.\nGemini 1.0, our first version, comes in three sizes: Ultra for highly-complex tasks, Pro for enhancedperformance and deployability at scale, and Nano for on-device applications. Each size is specificallytailored to address different computational limitations and application requirements.\nAfter large-scale pre-training, we post-train our models to improve overall quality, enhance targetcapabilities, and ensure alignment and safety criteria are met. Due to the varied requirements ofour downstream applications, we have produced two post-trained Gemini model family variants.Chat-focused variants, referred to as Gemini Apps models, are optimized for Gemini and GeminiAdvanced, our conversational AI service formerly known as Bard. Developer-focused variants, referredto as Gemini API models, are optimized for a range of products and are accessible through Google AIStudio and Cloud Vertex AI.\nWe evaluate the performance of pre- and post-trained Gemini models on a comprehensive suiteof internal and external benchmarks covering a wide range of language, coding, reasoning, andmultimodal tasks.\nThe Gemini family advances state-of-the-art in large-scale language modeling (Anil et al., 2023;Brown et al., 2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al.,2019; Rae et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiyet al., 2020; OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al.,2023; Zhang et al., 2023), and video understanding (Alayrac et al., 2022; Chen et al., 2023). Italso builds on the work on sequence models (Sutskever et al., 2014), a long history of work in deeplearning based on neural networks (LeCun et al., 2015), and machine learning distributed systems\n1See Contributions and Acknowledgments section for full author list.Please send correspondence to gemini-1-report@google.com\n© 2025 Google. All rights reserved\n\narXiv:2312.11805v5  [cs.CL]  9 May 2025"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p1_img1.png"},"content":"Gemini: A Family of Highly Capable\nMultimodal Models\nGemini Team, Google1\nThis report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities\nacross image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano\nsizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained\nuse-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model\nadvances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve\nhuman-expert "}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n(Barham et al., 2022; Bradbury et al., 2018; Dean et al., 2012) that enable large-scale training.\nOur most capable model, Gemini Ultra, achieves new state-of-the-art results in 30 of 32 benchmarkswe report on, including 10 of 12 popular text and reasoning benchmarks, 9 of 9 image understandingbenchmarks, 6 of 6 video understanding benchmarks, and 5 of 5 speech recognition and speechtranslation benchmarks. Gemini Ultra is the first model to achieve human-expert performance onMMLU (Hendrycks et al., 2021a) — a prominent benchmark testing knowledge and reasoning via asuite of exams — with a score above 90%. Beyond text, Gemini Ultra makes notable advances onchallenging multimodal reasoning tasks. For example, on the recent MMMU benchmark (Yue et al.,2023), that comprises questions about images on multi-discipline tasks requiring college-level subjectknowledge and deliberate reasoning, Gemini Ultra achieves a new state-of-the-art score of 62.4%,outperforming the previous best model by more than 5 percentage points. It provides a uniformperformance lift for video question answering and audio understanding benchmarks.\nQualitative evaluation showcases impressive crossmodal reasoning capabilities, enabling the modelto understand and reason across an input sequence of audio, images, and text natively (see Figure 5and Table 13). Consider the educational setting depicted in Figure 1 as an example. A teacher hasdrawn a physics problem of a skier going down a slope, and a student has worked through a solution toit. Using Gemini models’ multimodal reasoning capabilities, the model is able to understand the messyhandwriting, correctly understand the problem formulation, convert both the problem and solutionto mathematical typesetting, identify the specific step of reasoning where the student went wrong insolving the problem, and then give a worked through correct solution to the problem. This opens upexciting educational possibilities, and we believe the new multimodal and reasoning capabilities ofGemini models have dramatic applications across many fields.\nThe reasoning capabilities of large language models show promise toward building generalistagents that can tackle more complex multi-step problems. The AlphaCode team built AlphaCode2 (Leblond et al, 2023), a new Gemini-model-powered agent, that combines Gemini models’ rea-soning capabilities with search and tool-use to excel at solving competitive programming problems.AlphaCode 2 ranks within the top 15% of entrants on the Codeforces competitive programmingplatform, a large improvement over its state-of-the-art predecessor in the top 50% (Li et al., 2022).\nIn tandem, we advance the frontier of efficiency with Gemini Nano, a series of small modelstargeting on-device deployment. These models excel in on-device tasks, such as summarization,reading comprehension, text completion tasks, and exhibit impressive capabilities in reasoning, STEM,coding, multimodal, and multilingual tasks relative to their sizes.\nIn the following sections, we first provide an overview of the model architecture, training infras-tructure, and pre-training dataset. We then present detailed evaluations of the pre- and post-trainedGemini model family, covering well-studied benchmarks across text, code, image, audio and video —which include both English performance and multilingual capabilities. Next we discuss our approachto post-training, highlight common and distinct aspects of the Gemini Apps and Gemini API modelvariants, and benchmark their performance on key capabilities. Responsible deployment is critical: weexplain our process for impact assessments, developing model policies, evaluations, and mitigationsof harm before deployment decisions. Finally, we discuss the broader implications of Gemini models,their limitations alongside their potential applications — paving the way for a new era of researchand innovation in AI.\n2"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nFigure 1 | Verifying a student’s solution to a physics problem. The model is able to correctly recognizeall of the handwritten content and verify the reasoning. On top of understanding the text in theimage, it needs to understand the problem setup and correctly follow instructions to generate LATEX."}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"2. Model Architecture\nGemini models build on top of Transformer decoders (Vaswani et al., 2017b) that are enhancedwith improvements in architecture and model optimization to enable stable training at scale andoptimized inference on Google’s Tensor Processing Units. They are trained to support 32k contextlength, employing efficient attention mechanisms (for e.g. multi-query attention (Shazeer, 2019a)).Our first version, Gemini 1.0, comprises three main sizes to support a wide range of applications asdiscussed in Table 1.\nGemini models are trained to accommodate textual input interleaved with a wide variety of audioand visual inputs, such as natural images, charts, screenshots, PDFs, and videos, and they can producetext and image outputs (see Figure 2). The visual encoding of Gemini models is inspired by our ownfoundational work on Flamingo (Alayrac et al., 2022), CoCa (Yu et al., 2022a), and PaLI (Chen et al.,2022), with the important distinction that the models are multimodal from the beginning and cannatively output images using discrete image tokens (Ramesh et al., 2021; Yu et al., 2022b).\nVideo understanding is accomplished by encoding the video as a sequence of frames in the largecontext window. Video frames or images can be interleaved naturally with text or audio as part of themodel input. The models can handle variable input resolution in order to spend more compute ontasks that require fine-grained understanding. In addition, Gemini models can directly ingest audio\n3"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p3_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nFigure 1 | Verifying a student’s solution to a physics problem. The model is able to correctly recognize\nall of the handwritten content and verify the reasoning. On top of understanding the text in the\nimage, it needs to understand the problem setup and correctly follow instructions to generate LATEX.\n2. Model Architecture\nGemini models build on top of Transformer decoders (Vaswani et al., 2017b) that are enhanced\nwith improvements in architecture and model optimization to enable stable training at scale and\noptimized inference on Google’s T"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nModel sizeModel description\nUltraOur most capable model that delivers state-of-the-art performance across a widerange of highly complex tasks, including reasoning and multimodal tasks. It isefficiently serveable at scale on TPU accelerators due to the Gemini architecture.\nProA performance-optimized model in terms of cost as well as latency that deliverssignificant performance across a wide range of tasks. This model exhibits strongreasoning performance and broad multimodal capabilities.\nNanoOur most efficient model, designed to run on-device. We trained two versions ofNano, with 1.8B (Nano-1) and 3.25B (Nano-2) parameters, targeting low and highmemory devices respectively. It is trained by distilling from larger Gemini models. Itis 4-bit quantized for deployment and provides best-in-class performance.\nTable 1 | An overview of the Gemini 1.0 model family.\nFigure 2 | Gemini models support interleaved sequences of text, image, audio, and video as inputs(illustrated by tokens of different colors in the input sequence). They can output responses withinterleaved image and text.\nsignals at 16kHz from Universal Speech Model (USM) (Zhang et al., 2023) features. This enables themodel to capture nuances that are typically lost when the audio is naively mapped to a text input (forexample, see audio understanding demo on the website).\nTraining the Gemini family of models required innovations in training algorithms, dataset, andinfrastructure. For the Pro model, the inherent scalability of our infrastructure and learning algorithmsenable us to complete pre-training in a matter of weeks, leveraging a fraction of the Ultra’s resources.The Nano series of models leverage additional advancements in distillation and training algorithmsto produce the best-in-class small language models for a wide variety of tasks, such as summarizationand reading comprehension, which power our next generation on-device experiences.\n\n3. Training Infrastructure\nWe trained Gemini models using TPUv5e and TPUv4 (Jouppi et al., 2023), depending on their sizesand configuration. Training Gemini Ultra used a large fleet of TPUv4 accelerators owned by Google\n4"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p4_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nModel size\nModel description\nUltra\nOur most capable model that delivers state-of-the-art performance across a wide\nrange of highly complex tasks, including reasoning and multimodal tasks. It is\nefficiently serveable at scale on TPU accelerators due to the Gemini architecture.\nPro\nA performance-optimized model in terms of cost as well as latency that delivers\nsignificant performance across a wide range of tasks. This model exhibits strong\nreasoning performance and broad multimodal capabilities.\nNano\nOur most efficient model, designed to run o"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nacross multiple datacenters. This represents a significant increase in scale over our prior flagshipmodel PaLM-2 which presented new infrastructure challenges. Scaling up the number of acceleratorsresults in a proportionate decrease in the mean time between failure of hardware in the overall system.We minimized the rate of planned reschedules and preemptions, but genuine machine failures arecommonplace across all hardware accelerators at such large scales.\nTPUv4 accelerators are deployed in “SuperPods” of 4096 chips, each connected to a dedicatedoptical switch, which can dynamically reconfigure 4x4x4 chip cubes into arbitrary 3D torus topologiesin around 10 seconds (Jouppi et al., 2023). For Gemini Ultra, we decided to retain a small number ofcubes per superpod to allow for hot standbys and rolling maintenance.\nTPU accelerators primarily communicate over the high speed inter-chip-interconnect, but atGemini Ultra scale, we combine SuperPods in multiple datacenters using Google’s intra-cluster andinter-cluster network (Poutievski et al., 2022; Wetherall et al., 2023; yao Hong et al., 2018). Google’snetwork latencies and bandwidths are sufficient to support the commonly used synchronous trainingparadigm, exploiting model parallelism within superpods and data-parallelism across superpods.\nThe ‘single controller’ programming model of Jax (Bradbury et al., 2018) and Pathways (Barhamet al., 2022) allows a single Python process to orchestrate the entire training run, dramaticallysimplifying the development workflow. The GSPMD partitioner (Xu et al., 2021) in the XLA compilerpartitions the training step computation, and the MegaScale XLA compiler (XLA, 2019) pass staticallyschedules appropriate collectives so that they maximally overlap with the computation with very littlevariation in step time.\nMaintaining a high goodput2at this scale would have been impossible using the conventionalapproach of periodic checkpointing of weights to persistent cluster storage. For Gemini models, weinstead made use of redundant in-memory copies of the model state, and on any unplanned hardwarefailures, we rapidly recover directly from an intact model replica. Compared to both PaLM and PaLM-2(Anil et al., 2023), this provided a substantial speedup in recovery time, despite the significantlylarger training resources being used. As a result, the overall goodput for the largest-scale training jobincreased from 85% to 97%.\nTraining at unprecedented scale invariably surfaces new and interesting systems failure modes -and in this instance one of the problems that we needed to address was that of “Silent Data Corruption(SDC)” (Dixit et al., 2021; Hochschild et al., 2021; Vishwanathan et al., 2015). Although these areextremely rare, the scale of Gemini models means that we can expect SDC events to impact trainingevery week or two. Rapidly detecting and removing faulty hardware required several new techniquesthat exploit deterministic replay to isolate incorrect computations, combined with proactive SDCscanners on idle machines and hot standbys. Our fully deterministic infrastructure allowed us toquickly identify root causes (including hardware failures) during the development leading up to theUltra model, and this was a crucial ingredient towards stable training."}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"4. Pre-Training Dataset\nGemini models are trained on a dataset that is both multimodal and multilingual. Our pre-trainingdataset uses data from web documents, books, and code, and includes image, audio, and video data.\nWe use the SentencePiece tokenizer (Kudo and Richardson, 2018) and find that training thetokenizer on a large sample of the entire training corpus improves the inferred vocabulary andsubsequently improves model performance. For example, we find Gemini models can efficiently\n2We define goodput as the time spent computing useful new steps over the elapsed time of the training job.\n5"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\ntokenize non-Latin scripts which can, in turn, benefit model quality as well as training and inferencespeed.\nThe number of tokens used to train the largest models were determined following the approachin Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improveperformance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a).\nWe apply quality filters to all datasets, using both heuristic rules and model-based classifiers.We also perform safety filtering to remove harmful content based on our policies. To maintain theintegrity of evaluations, we search for and remove any evaluation data that may have been in ourtraining corpus before using data for training. The final data mixtures and weights were determinedthrough ablations on smaller models. We stage training to alter the mixture composition duringtraining – increasing the weight of domain-relevant data towards the end of training. We find thatdata quality is an important factor for highly-performing models, and believe that many interestingquestions remain around finding the optimal dataset distribution for pre-training."}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"5. Evaluation\nThe Gemini models are natively multimodal, as they are trained jointly across text, image, audio,and video. One open question is whether this joint training can result in a model which has strongcapabilities in each domain – even when compared to models and approaches that are narrowlytailored to single domains. We find this to be the case: Gemini models set a new state of the artacross a wide range of text, image, audio, and video benchmarks. ww\n5.1. Text\n5.1.1. Academic Benchmarks\nWe compare pre- and post-trained Gemini Pro and Ultra models to a suite of external LLMs and ourprevious best model PaLM 2 across a series of text-based academic benchmarks covering reasoning,reading comprehension, STEM, and coding. We report these results in Table 2. Broadly, we findthat the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 andperforms comparably with several of the most capable models available, and Gemini Ultra outperformsall current models. In this section, we examine some of these findings.\nOn MMLU (Hendrycks et al., 2021a), Gemini Ultra can outperform all existing models, achievingan accuracy of 90.04%. MMLU is a holistic exam benchmark, which measures knowledge across aset of 57 subjects. Human expert performance is gauged at 89.8% by the benchmark authors, andGemini Ultra is the first model to exceed this threshold, with the prior state-of-the-art result at 86.4%.Achieving high performance requires specialist knowledge across many domains (e.g. law, biology,history, etc.), alongside reading comprehension and reasoning. We find Gemini Ultra achieves highestaccuracy when used in combination with a chain-of-thought prompting approach (Wei et al., 2022b)that accounts for model uncertainty. The model produces a chain of thought with k samples, forexample 8 or 32. If there is a consensus above a preset threshold (selected based on the validationsplit), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihoodchoice without chain of thought. We refer the reader to appendix for a detailed breakdown of howthis approach compares with only chain-of-thought prompting or only greedy sampling.\nIn mathematics, a field commonly used to benchmark the analytical capabilities of models, GeminiUltra shows strong performance on both elementary exams and competition-grade problem sets. Forthe grade-school math benchmark, GSM8K (Cobbe et al., 2021), we find Gemini Ultra reaches 94.4%\n6"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\naccuracy with chain-of-thought prompting and self-consistency (Wang et al., 2022) compared tothe previous best accuracy of 92% with the same prompting technique. Similar positive trends areobserved in increased difficulty math problems drawn from middle- and high-school math competitions(MATH benchmark), with the Gemini Ultra model outperforming all competitor models, reaching53.2% using 4-shot prompting. The model also outperforms the state of the art on even harder tasksderived from American Mathematical Competitions (150 questions from 2022 and 2023). Smallermodels perform poorly on this challenging task scoring close to random, but Gemini Ultra can solve32% of the questions, compared to the 30% solve rate for GPT-4.\nGemini Ultra also excels in coding, a popular use case of current LLMs. We evaluate the modelon many conventional and internal benchmarks and also measure its performance as part of morecomplex reasoning systems such as AlphaCode 2 (see Section 5.1.7 on complex reasoning systems).For example, on HumanEval, a standard code-completion benchmark (Chen et al., 2021) mappingfunction descriptions to Python implementations, instruction-tuned Gemini Ultra correctly implements74.4% of problems. On a new held-out evaluation benchmark for python code generation tasks,Natural2Code, where we ensure no web leakage, Gemini Ultra achieves the highest score of 74.9%.\nEvaluation on these benchmarks is challenging and may be affected by data contamination. Weperformed an extensive leaked data analysis after training to ensure the results we report here are asscientifically sound as possible, but still found some minor issues and decided not to report results one.g. LAMBADA (Paperno et al., 2016). As part of the evaluation process, on a popular benchmark,HellaSwag (Zellers et al., 2019), we find that an additional hundred fine-tuning steps on specificwebsite extracts corresponding to the HellaSwag training set (which were not included in the Geminimodel pretraining set) improve the validation accuracy of Gemini Pro to 89.6% and Gemini Ultra to96.0%, when measured with 1-shot prompting (we measured GPT-4 obtained 92.3% when evaluated1-shot via the API). This suggests that the benchmark results are susceptible to the pretraining datasetcomposition. We choose to report HellaSwag decontaminated results only in a 10-shot evaluationsetting. We believe there is a need for more robust and nuanced standardized evaluation benchmarkswith no leaked data. So, we evaluate Gemini models on several new held-out evaluation datasetsthat were recently released, such as WMT23 and Math-AMC 2022-2023 problems, or internallygenerated from non-web sources, such as Natural2Code. We refer the reader to Appendix 10.3 for acomprehensive list of our evaluation benchmarks.\nEven so, model performance on these benchmarks gives us an indication of the model capabilitiesand where they may provide impact on real-world tasks. For example, Gemini Ultra’s impressivereasoning and STEM competencies pave the way for advancements in LLMs within the educationaldomain3. The ability to tackle complex mathematical and scientific concepts opens up excitingpossibilities for personalized learning and intelligent tutoring systems.\n5.1.2. Trends in Capabilities\nWe investigate the trends in capabilities across the Gemini model family by evaluating them on aholistic harness of more than 50 benchmarks in six different capabilities, noting that some of themost notable benchmarks were discussed in the last section. These capabilities are: “Factuality”covering open/closed-book retrieval and question answering tasks; “Long-Context” covering long-form summarization, retrieval and question answering tasks; “Math/Science” including tasks formathematical problem solving, theorem proving, and scientific exams; “Reasoning” tasks that requirearithmetic, scientific, and commonsense reasoning; “Multilingual” tasks for translation, summarization,and reasoning in multiple languages. Several of these capabilities are targeted by post-training(Section 6). Please see Appendix 10.3 for a detailed list of tasks included for each capability.\n3See demos on website https://deepmind.google/gemini.\n7"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nGeminiUltraGeminiPro\nGPT-4GPT-3.5PaLM 2-LClaude 2Inflect-ion-2Grok 1LLAMA-2\nMMLUMultiple-choice questionsin 57 subjects(professional &academic)(Hendrycks et al., 2021a)\n90.04%CoT@32∗\n83.7%5-shot\n79.13%CoT@8∗\n71.8%5-shot\n87.29%CoT@32(via API∗∗)\n86.4%5-shot(reported)\n70%5-shot78.4%5-shot78.5%5-shot CoT79.6%5-shot73.0%5-shot68.0%∗∗∗\nGSM8KGrade-school math(Cobbe et al., 2021)\n94.4%Maj1@3286.5%Maj1@32\n92.0%SFT &5-shot CoT\n57.1%5-shot80.0%5-shot88.0%0-shot81.4%8-shot62.9%8-shot56.8%5-shot\nMATHMath problems across5 difficulty levels &7 subdisciplines(Hendrycks et al., 2021b)\n53.2%4-shot32.6%4-shot\n52.9%4-shot(via API∗∗)\n50.3%(Zheng et al.,2023)\n34.1%4-shot(via API∗∗)\n34.4%4-shot—34.8%23.9%4-shot13.5%4-shot\nBIG-Bench-HardSubset of hard BIG-benchtasks written as CoT prob-lems(Srivastava et al., 2022)\n83.6%3-shot75.0%3-shot\n83.1%3-shot(via API∗∗)\n66.6%3-shot(via API∗∗)\n77.7%3-shot———51.2%3-shot\nHumanEvalPython coding tasks(Chen et al., 2021)\n74.4%0-shot(PT∗∗∗∗)\n67.7%0-shot(PT∗∗∗∗)\n67.0%0-shot(reported)\n48.1%0-shot—70.0%0-shot44.5%0-shot63.2%0-shot29.9%0-shot\nNatural2CodePython code generation.(New held-out set with noleakage on web)\n74.9%0-shot69.6%0-shot\n73.9%0-shot(via API∗∗)\n62.3%0-shot(via API∗∗)\n—————\nDROPReading comprehension& arithmetic.(metric: F1-score)(Dua et al., 2019)\n82.4Variableshots\n74.1Variableshots\n80.93-shot(reported)\n64.13-shot82.0Variableshots\n————\nHellaSwag(validation set)Common-sense multiplechoice questions(Zellers et al., 2019)\n87.8%10-shot84.7%10-shot\n95.3%10-shot(reported)\n85.5%10-shot86.8%10-shot—89.0%10-shot—80.0%∗∗∗\nWMT23Machine translation (met-ric: BLEURT)(Tom et al., 2023)\n74.41-shot(PT∗∗∗∗)\n71.71-shot\n73.81-shot(via API∗∗)\n—72.71-shot————\nTable 2 | Gemini performance on text benchmarks with external comparisons and PaLM 2-L.∗The model produces a chain of thought with k = 8 or 32 samples, if there is a consensus above a threshold (chosen based on the validationsplit), it selects this answer, otherwise it reverts to a greedy sample. Further analysis in Appendix 10.2.∗∗Results self-collected via the API in Nov, 2023.∗∗∗Results shown use the decontaminated numbers from Touvron et al. (2023b) report as the most relevant comparison to Gemini modelswhich have been decontaminated as well.)∗∗∗∗PT denotes a post-trained Gemini API model.\nWe observe consistent quality gains with increased model size in Figure 3, especially in reasoning,math/science, summarization and long-context. Gemini Ultra is the best model across the board forall six capabilities. Gemini Pro, the second-largest model in the Gemini family of models, is also quitecompetitive while being a lot more efficient to serve."}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"5.1.3. Nano\nBringing AI closer to the user, we discuss the Gemini Nano 1 and Nano 2 models engineered foron-device deployments. These models excel in summarization and reading comprehension tasks withper-task fine-tuning. Figure 3 shows the performance of these pre-trained models in comparisonto the much larger Gemini Pro model, while Table 3 dives deeper into specific factuality, coding,Math/Science, and reasoning tasks. Nano-1 and Nano-2 model sizes are only 1.8B and 3.25Bparameters respectively. Despite their size, they show exceptionally strong performance on factuality,i.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and\n8"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nFactuality\nLong-Context\nMath/Science\nSummarization\nReasoning\nMultilinguality\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nNormalized Performance vs Pro\nNano 1Nano 2ProUltra\nFigure 3 | Language understanding and generation performance of Gemini model family acrossdifferent capabilities (normalized by the Gemini Pro model).\nmultilingual tasks. With new capabilities accessible to a broader set of platforms and devices, theGemini models expand accessibility to everyone.\nGemini Nano 1Gemini Nano 2\naccuracynormalizedby Proaccuracynormalizedby Pro\nBoolQ71.60.8179.30.90TydiQA (GoldP)68.90.8574.20.91NaturalQuestions (Retrieved)38.60.6946.50.83NaturalQuestions (Closed-book) 18.80.4324.80.56BIG-Bench-Hard (3-shot)34.80.4742.40.58MBPP20.00.3327.20.45MATH (4-shot)13.50.4122.80.70MMLU (5-shot)45.90.6455.80.78\nTable 3 | Performance of Gemini Nano series on factuality, summarization, reasoning, coding andSTEM tasks compared to significantly larger Gemini Pro model.\n5.1.4. Multilinguality\nThe multilingual capabilities of the Gemini models are evaluated using a diverse set of tasks requir-ing multilingual understanding, cross-lingual generalization, and the generation of text in multiplelanguages. These tasks include machine translation benchmarks (WMT 23 for high-medium-lowresource translation; Flores, NTREX for low and very low resource languages), summarization bench-marks (XLSum, Wikilingua), and translated versions of common benchmarks (MGSM: professionallytranslated into 11 languages).\n5.1.4.1Machine Translation\nTranslation is a canonical benchmark in machine learning with a rich history. We evaluated a post-trained Gemini API Ultra model (see Section 6.5.3) on the entire set of language pairs in the WMT 23translation benchmark in a few-shot setting. Overall, we found that Gemini Ultra (and other Geminimodels) performed remarkably well at translating from English to any other language, and surpassed\n9"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nthe LLM-based translation methods when translating out-of-English, on high-resource, mid-resourceand low-resource languages. In the WMT 23 out-of-English translation tasks, Gemini Ultra achievedthe highest LLM-based translation quality, with an average BLEURT (Sellam et al., 2020) score of 74.8,compared to GPT-4’s score of 73.6, and PaLM 2’s score of 72.2. When averaged across all languagepairs and directions for WMT 23, we see a similar trend with Gemini Ultra 74.4, GPT-4 73.8 andPaLM 2-L 72.7 average BLEURT scores on this benchmark.\nWMT 23(Avg BLEURT)Gemini UltraGemini ProGemini Nano 2Gemini Nano 1GPT-4PaLM 2-L\nHigh Resource74.271.767.764.174.072.6Mid Resource74.771.867.064.873.672.7Out-of-English74.871.566.265.273.672.2Into-English73.972.069.063.574.173.4All languages74.471.767.464.873.872.7\nTable 4 | Performance of Gemini models on WMT 23 translation benchmark. All numbers with 1-shot.\nIn addition to the languages and translation tasks above, we also evaluate Gemini Ultra on verylow-resource languages. These languages were sampled from the tail of the following language sets:Flores-200 (Tamazight and Kanure), NTREX (North Ndebele), and an internal benchmark (Quechua).For these languages, both from and into English, Gemini Ultra achieved an average chrF score of 27.0in 1-shot setup, while the next-best model, PaLM 2-L, achieved a score of 25.3.\n5.1.4.2Multilingual Math and Summarization\nBeyond translation, we evaluated how well Gemini models perform in challenging tasks across arange of languages. We specifically investigated the math benchmark MGSM (Shi et al., 2023), whichis a translated variant of the math benchmark GSM8K (Cobbe et al., 2021). We find Gemini Ultraachieves an accuracy of 79.0%, an advance over PaLM 2-L which scores 74.7%, when averagedacross all languages in an 8-shot setup. We also benchmark Gemini models on the multilingualsummarization benchmarks – XLSum (Hasan et al., 2021) and WikiLingua (Ladhak et al., 2020). InXLSum, Gemini Ultra reached an average of 17.6 rougeL score compared to 15.4 for PaLM 2. ForWikilingua, Gemini Ultra (5-shot) trails behind PaLM 2 (3-shot) measured in BLEURT score. SeeTable 5 for the full results. Overall the diverse set of multilingual benchmarks show that Geminifamily models have a broad language coverage, enabling them to also reach locales and regions withlow-resource languages.\nGemini UltraGemini ProGPT-4PaLM 2-L\nMGSM(8-shot)79.063.574.574.7XLsum(3-shot)17.616.2—15.4Wikilingua48.947.8—50.4\nTable 5 | Performance of Gemini models on multilingual math and summarization.\n5.1.5. Long Context\nGemini models are trained with a sequence length of 32,768 tokens and we find that they make useof their context length effectively. We first verify this by running a synthetic retrieval test: we placekey-value pairs at the beginning of the context, then add long filler text, and ask for value associatedwith a particular key. We find that the Ultra model retrieves the correct value with 98% accuracywhen queried across the full context length. We further investigate this by plotting the negative log\n10"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nlikelihood (NLL) versus the token index across a held-out set of long documents in Figure 4. Wefind that the NLL decreases with sequence position up to the full 32K context length. The longercontext length of Gemini models enable new use cases such as retrieval over documents and videounderstanding discussed in Section 5.2.2.\n81632641282565121K2K4K8K16K32KSequence position\nNLL\nProUltra\nFigure 4 | Negative log likelihood as a function of token index across 32K context length on a held-outset of long documents.\n5.1.6. Factuality\nFactuality (Maynez et al., 2020) is a key focus of our model’s training and deployment. We evaluatethree aspects of factuality for our Gemini API models:\n1. Closed-Book Factuality: If provided with a fact-seeking prompt without any given source,Gemini API models should not hallucinate incorrect information (see Section 2 of Roberts et al.(2020) for a definition). These prompts can range from information-seeking prompts (e.g. “Whois the prime minister of India?”) to semi-creative prompts that may request factual information(e.g. “Write a 500-word speech in favor of the adoption of renewable energy”).2. Attribution: If instructed to generate a response grounded to a given context, we aim to ensurethat Gemini API models produce a response with the highest degree of faithfulness to thecontext (Maynez et al., 2020; Rashkin et al., 2023). This may include the summarization of auser-provided source, generating fine-grained citations given a question and provided snippetsakin to Menick et al. (2022); Peng et al. (2023), answering questions from a long-form sourcesuch as a book (Mihaylov et al., 2018), and transforming a given source to a desired output(e.g. an email from a portion of a meeting transcript).3. Hedging: If prompted with an input that is “unanswerable”, Gemini API models must ac-knowledge that it cannot provide a response by hedging to avoid hallucination. These includescenarios where the input prompt contains false-premise questions [see examples in Hu et al.(2023)], the input prompt instructs the model to perform open book QA, but the answer is notderivable from the given context, and so forth.\nFactuality is evaluated via human annotators who fact-check each response manually; we reportthe percentage of factually inaccurate responses as judged by annotators. Attribution is evaluated viahuman annotators who check for attribution to sources in the prompt for each response manually;the reported metric is AIS (Rashkin et al., 2023). For hedging, we use an automatic evaluation setupwhere we measure whether models hedge accurately.\nWe compare Gemini API Pro with a version without any factuality-focused adaptation in Table 6.We see that the rate of inaccuracy is halved in the factuality set, the accuracy of attribution is increased\n11"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nby 50% from the attribution set, and the model successfully hedges 70% (up from 0%) in the providedhedging set task.\nFactuality(Inaccurate Rate)Attribution(AIS)Hedging(Accuracy)\nGemini API ProNo factuality-focused adaptation6.7%[5.8%, 7.8%]40.2%[37.9%, 42.5%]0%\nGemini API ProFinal stage of post-training3.8%[3.1%, 4.8%]60.0%[57.6%, 62.1%]69.3%\nTable 6 | Factuality mitigations: Impact of post-training on the rate of inaccuracy, presence of attributionand the rate of accurate hedging on Gemini API Pro (with corresponding 95% confidence intervals).\n5.1.7. Complex Reasoning Systems\nGemini models can also be combined with additional techniques such as search and tool-use to createpowerful reasoning systems that can tackle more complex multi-step problems. One example of sucha system is AlphaCode 2, a new state-of-the-art agent that excels at solving competitive programmingproblems (Leblond et al, 2023). AlphaCode 2 uses a specialized version of Gemini Pro – tuned oncompetitive programming data similar to the data used in Li et al. (2022) – to conduct a massivesearch over the space of possible programs. This is followed by a tailored filtering, clustering andreranking mechanism. Gemini Pro is fine-tuned both to be a coding model to generate proposalsolution candidates, and to be a reward model that is leveraged to recognize and extract the mostpromising code candidates.\nAlphaCode 2 is evaluated on Codeforces,4the same platform as AlphaCode, on 12 contests fromdivision 1 and 2, for a total of 77 problems. AlphaCode 2 solved 43% of these competition problems, a1.7x improvement over the prior record-setting AlphaCode system which solved 25%. Mapping this tocompetition rankings, AlphaCode 2 built on top of Gemini Pro sits at an estimated 85th percentile onaverage – i.e. it performs better than 85% of entrants. This is a significant advance over AlphaCode,which only outperformed 50% of competitors.\nThe composition of powerful pre-trained models with search and reasoning mechanisms is anexciting direction towards more general agents; another key ingredient is deep understanding acrossa range of modalities which we discuss in the next section.\n4http://codeforces.com/\n12"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n\n5.2. Multimodal\nGemini models are natively multimodal. These models exhibit the unique ability to seamlesslycombine their capabilities across modalities (e.g. extracting information and spatial layout out ofa table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. itsstate-of-art-performance in math and coding) as seen in examples in Figures 5 and 14. The modelsalso show strong performance in discerning fine-grained details in inputs, aggregating context acrossspace and time, and applying these capabilities over a temporally-related sequence of video framesand/or audio inputs.\nThe sections below provide more detailed evaluation of the model across different modalities(image, video, and audio), together with qualitative examples of the model’s capabilities for imagegeneration and the ability to combine information across different modalities."}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"5.2.1. Image Understanding\nWe evaluate post-trained Gemini API models on four different capabilities: high-level object recogni-tion using captioning or question-answering tasks such as VQAv2; fine-grained transcription usingtasks such as TextVQA and DocVQA requiring the model to recognize low-level details; chart un-derstanding requiring spatial understanding of input layout using ChartQA and InfographicVQAtasks; and multimodal reasoning using tasks such as Ai2D, MathVista and MMMU. For zero-shot QAevaluation, the model is instructed to provide short answers aligned with the specific benchmark. Allnumbers are obtained using greedy sampling and without any use of external OCR tools.\nGeminiUltra(pixel only)\nGeminiPro(pixel only)\nGeminiNano 2(pixel only)\nGeminiNano 1(pixel only)\nGPT-4VPrior SOTA\nMMMU (val)Multi-discipline college-level problems(Yue et al., 2023)\n59.4%pass@1\n62.4%Maj1@32\n47.9%32.6%26.3%56.8%56.8%GPT-4V, 0-shot\nTextVQA (val)Text reading on natural images(Singh et al., 2019)\n82.3%74.6%65.9%62.5%78.0%79.5%Google PaLI-3, fine-tuned\nDocVQA (test)Document understanding(Mathew et al., 2021)\n90.9%88.1%74.3%72.2%88.4%(pixel only)88.4%GPT-4V, 0-shot\nChartQA (test)Chart understanding(Masry et al., 2022)\n80.8%74.1%51.9%53.6%78.5%(4-shot CoT)79.3%Google DePlot, 1-shot PoT(Liu et al., 2023)\nInfographicVQA (test)Infographic understanding(Mathew et al., 2022)\n80.3%75.2%54.5%51.1%75.1%(pixel only)75.1%GPT-4V, 0-shot\nMathVista (testmini)Mathematical reasoning(Lu et al., 2023)\n53.0%45.2%30.6%27.3%49.9%49.9%GPT-4V, 0-shot\nAI2D (test)Science diagrams(Kembhavi et al., 2016)\n79.5%73.9%51.0%37.9%78.2%81.4%Google PaLI-X, fine-tuned\nVQAv2 (test-dev)Natural image understanding(Goyal et al., 2017)\n77.8%71.2%67.5%62.7%77.2%86.1%Google PaLI-X, fine-tuned\nTable 7 | Image understanding Gemini Ultra consistently outperforms existing approaches even inzero-shot, especially for OCR-related image understanding tasks for natural images, text, documents,and figures without using any external OCR engine (‘pixel only’). Many existing approaches fine-tuneon the respective tasks, highlighted in gray, which makes the comparison with 0-shot not apples-to-apples.\n13"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nWe find that Gemini Ultra is state of the art across a wide range of image-understanding bench-marks in Table 7. It achieves strong performance across a diverse set of tasks such as answeringquestions on natural images and scanned documents as well as understanding infographics, chartsand science diagrams. When compared against publicly reported results from other models (mostnotably GPT-4V), the Gemini model is better in zero-shot evaluation by a significant margin. It alsoexceeds several existing models that are specifically fine-tuned on the benchmark’s training sets forthe majority of tasks. The capabilities of the Gemini models lead to significant improvements in thestate of the art on academic benchmarks like MathVista (+3.1%)5or InfographicVQA (+5.2%).\nMMMU (Yue et al., 2023) is a recently released evaluation benchmark, which consists of questionsabout images across 6 disciplines with multiple subjects within each discipline that require college-level knowledge to solve these questions. Gemini Ultra achieves the best score on this benchmarkadvancing the state-of-the-art result by more than 5 percentage points and outperforms the previousbest result in 5 of 6 disciplines (see Table 8), thus showcasing its multimodal reasoning capabilities.\nMMMU (val)Gemini Ultra (0-shot)GPT-4V (0-shot)\nMaj@32pass@1pass@1\nArt & Design74.270.065.8Business62.756.759.3Science49.348.054.7Health & Medicine71.367.364.7Humanities & Social Science78.378.372.5Technology & Engineering53.047.136.7\nOverall62.459.456.8\nTable 8 | Gemini Ultra performance on the MMMU benchmark (Yue et al., 2023) per discipline.Each discipline covers multiple subjects, requiring college-level knowledge and complex reasoning.\nGemini models are also capable of operating across modalities and a diverse set of global languagessimultaneously, both for image understanding tasks (e.g., images containing text in Icelandic) and forgeneration tasks (e.g., generating image descriptions for a wide range of languages). We evaluate theperformance of generating image descriptions on a selected subset of languages in the Crossmodal-3600 (XM-3600) benchmark in a 4-shot setting, using the Flamingo evaluation protocol (Alayracet al., 2022), without any fine-tuning for all models. As shown in Table 9, Gemini models achieve asignificant improvement over the existing best model, Google PaLI-X.\nXM-3600 (CIDER)Gemini Ultra4-shotGemini Pro4-shotGoogle PaLI-X4-shot\nEnglish86.487.177.8French77.976.762.5Hindi31.129.822.2Modern Hebrew54.552.638.7Romanian39.037.730.2Thai86.777.056.0Chinese33.330.227.7\nAverage (of 7)58.455.945.0\nTable 9 | Multilingual image understanding Gemini models outperform existing models in captioningimages in many languages when benchmarked on a subset of languages in XM-3600 dataset (Thapliyalet al., 2022).\n5MathVista is a comprehensive mathematical reasoning benchmark consisting of 28 previously published multimodaldatasets and three newly created datasets. Our MathVista results were obtained by running the MathVista authors’evaluation script.\n14"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nFigure 5 | Using Gemini models’ multimodal reasoning capabilities to generate matplotlib codefor rearranging the subplots. The multimodal prompt is shown at the top-left in gray. Gemini Ultra’sresponse, including its generated code, is shown in the right column in blue. The bottom left figureshows rendered version of the generated code. Successfully solving this task shows the model’scapability to combine several capabilities: (1) recognition of the functions depicted in the plots; (2)inverse graphics to infer the code that would have generated the subplots; (3) instruction-followingto put subplots in their desired positions; and (4) abstract reasoning to infer that the exponential plotmust stay in its original place, because the sine plot must move out of the way for the 3-dimensionalplot.\nQualitative evaluation in Figure 5 illustrates an example of Gemini Ultra’s multimodal reasoningcapabilities. The model is required to solve the task of generating matplotlib code that would rearrange\n15"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p15_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nFigure 5 | Using Gemini models’ multimodal reasoning capabilities to generate matplotlib code\nfor rearranging the subplots. The multimodal prompt is shown at the top-left in gray. Gemini Ultra’s\nresponse, including its generated code, is shown in the right column in blue. The bottom left figure\nshows rendered version of the generated code. Successfully solving this task shows the model’s\ncapability to combine several capabilities: (1) recognition of the functions depicted in the plots; (2)\ninverse graphics to infer the code that would have g"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\na set of subplots provided by the user. The model output shows that it successfully solves this taskcombining multiple capabilities of understanding the user plot, inferring the code required to generateit, following user instructions to put subplots in their desired positions, and abstract reasoning aboutthe output plot. This highlights Gemini Ultra’s native multimodality and alludes to its more complexreasoning abilities across interleaved sequences of image and text. We refer the reader to the appendixfor more qualitative examples.\n5.2.2. Video Understanding\nUnderstanding video input is an important step towards a useful generalist agent. We measure thevideo understanding capability across several established benchmarks that are held-out from training.These tasks measure whether the model is able to understand and reason over a temporally-relatedsequence of frames. For each video task, we sample 16 equally-spaced frames from each video clipand feed them to the Gemini models. For the YouTube video datasets (all datasets except NextQAand the Perception test), we evaluate the Gemini models on videos that were still publicly availablein the month of November, 2023.\nGemini Ultra achieves state-of-the-art performance on various few-shot video captioning tasksas well as zero-shot video question answering tasks as shown in Table 10. This demonstrates itscapability of strong temporal reasoning across several frames. Figure 23 in the appendix provides aqualitative example of understanding the video of the ball-striking mechanics of a soccer player andreasoning about the player can improve their game.\nTaskGemini UltraGemini ProFew-shot SoTA\nVATEX (test)62.757.456.0\nEnglish video captioning(Wang et al., 2019)4-shots4-shotsDeepMind Flamingo, 4-shots\nVATEX ZH (test)51.350.0–\nChinese video captioning(Wang et al., 2019)4-shots4-shots\nYouCook2 (val)135.4123.274.5\nEnglish cooking video captioning(Zhou et al., 2018)4-shots4-shotsDeepMind Flamingo, 4-shots\nNextQA (test)29.928.026.7\nVideo question answering(Xiao et al., 2021)0-shot0-shotDeepMind Flamingo, 0-shot\nActivityNet-QA (test)52.249.845.3\nVideo question answering(Yu et al., 2019)0-shot0-shotVideo-LLAVA, 0-shot\nPerception Test MCQA (test)54.751.146.3\nVideo question answering(Pătrăucean et al., 2023)0-shot0-shotSeViLA (Yu et al., 2023), 0-shot\nTable 10 | Few-shot video understanding across tasks and languages on selected academicbenchmarks. The reported metric is CIDER for video captioning, WUPS for NextQA, and top-1accuracy for the Perception Test and ActivityNet-QA. For ActivityNet-QA, we use the Video-LLAVA(Lin et al., 2023) evaluation protocol.\n5.2.3. Image Generation\nGemini models are able to output images natively, without having to rely on an intermediate naturallanguage description that can bottleneck the model’s ability to express images. This uniquely enablesthe model to generate images with prompts using interleaved sequences of image and text in a\n16"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nfew-shot setting. For example, the user might prompt the model to design suggestions of images andtext for a blog post or a website (see Figure 12 in the appendix).\nFigure 6 shows an example of image generation in 1-shot setting. Gemini Ultra model is promptedwith one example of interleaved image and text where the user provides two colors (blue and yellow)and image suggestions of creating a cute blue cat or a blue dog with yellow ear from yarn. Themodel is then given two new colors (pink and green) and asked for two ideas about what to createusing these colors. The model successfully generates an interleaved sequence of images and text withsuggestions to create a cute green avocado with pink seed or a green bunny with pink ears from yarn.\nFigure 6 | Image Generation. Gemini models can output multiple images interleaved with text givena prompt composed of image and text. In the left figure, Gemini Ultra is prompted in a 1-shot settingwith a user example of generating suggestions of creating cat and dog from yarn when given twocolors, blue and yellow. Then, the model is prompted to generate creative suggestions with two newcolors, pink and green, and it generates images of creative suggestions to make a cute green avocadowith pink seed or a green bunny with pink ears from yarn as shown in the right figure.\n17"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p17_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nfew-shot setting. For example, the user might prompt the model to design suggestions of images and\ntext for a blog post or a website (see Figure 12 in the appendix).\nFigure 6 shows an example of image generation in 1-shot setting. Gemini Ultra model is prompted\nwith one example of interleaved image and text where the user provides two colors (blue and yellow)\nand image suggestions of creating a cute blue cat or a blue dog with yellow ear from yarn. The\nmodel is then given two new colors (pink and green) and asked for two ideas about what to "}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n5.2.4. Audio Understanding\nWe evaluate the Gemini Nano-1 and Gemini Pro models on a variety of public benchmarks andcompare it with Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (large-v2 (Radfordet al., 2023) or large-v3 (OpenAI, 2023) as indicated). These benchmarks include automatic speechrecognition (ASR) tasks such as FLEURS (Conneau et al., 2023), VoxPopuli, (Wang et al., 2021),Multi-lingual Librispeech (Pratap et al., 2020), as well as the speech translation task CoVoST 2,translating different languages into English (Wang et al., 2020). We also report on an internalbenchmark YouTube test set. ASR tasks report a word error rate (WER) metric, where a lower numberis better. Translation tasks report a BiLingual Evaluation Understudy (BLEU) score, where a highernumber is better. FLEURS is reported on 62 languages that have language overlap with the trainingdata. Four segmented languages (Mandarin, Japanese, Korean and Thai) report character error rate(CER), instead of WER, similar to Whisper (Radford et al., 2023).\nTable 11 indicates that our Gemini Pro model significantly outperforms the USM and Whispermodels across all ASR and AST tasks, both for English and multilingual test sets. Note that there is alarge gain in FLEURS, compared to USM and Whisper, as our model is also trained with the FLEURStraining dataset. However, training the same model without FLEURS dataset results in a WER of 15.8,which still outperforms Whisper. Gemini Nano-1 model also outperforms both USM and Whisper onall datasets except FLEURS. Note that we did not evaluate Gemini Ultra on audio yet, though weexpect better performance from increased model scale.\nTaskMetricGeminiProGeminiNano-1Whisper(OpenAI, 2023;Radford et al.,2023)\nUSM(Zhangetal.,2023)\nAutomatic SpeechRecognitionYouTube(en-us)WER (↓)4.9%5.5%6.5%(v3)6.2%\nMultilingualLibrispeech(en-us)(Pratap et al., 2020)\nWER (↓)4.8%5.9%6.2%(v2)7.0 %\nFLEURS(62 lang)(Conneau et al., 2023)\nWER (↓)7.6%14.2%17.6%(v3)11.8%\nVoxPopuli(14 lang)(Wang et al., 2021)\nWER (↓)9.1%9.5%15.9%(v2)13.4%\nAutomatic SpeechTranslationCoVoST 2(21 lang)(Wang et al., 2020)\nBLEU (↑)40.135.429.1(v2)30.7\nTable 11 | Speech evaluation results on selected benchmarks for ASR and AST. For ASR, the reportedmetric is WER where lower is better. For AST, the reported metric is BLEU where higher is better.\nTable 12 shows further error analysis with USM and Gemini Pro. We find that Gemini Pro producesmore understandable responses, particularly on rare words and proper nouns.\nDomainTruthUSMGemini ProWav\nFleursScotturb bus 403 travels regularly toSintra, stopping at Cabo da Roca.\nScotboard bus four3 traversed regu-larly to Centra stopping at Cabo deRoga.\nScotturb bus 403 travels regularly toSintra, stopping at Cabo da Roca.\n\n▲"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":")))\nFleursThe archipelago lies 120 km northof the Peninsula. The largest is KingGeorge Island, with the settlement ofVilla Las Estrellas.\nThe archipelago lines 120 km northof peninsula. The largest is Kingurchisland with the settlement of CuaLosas.\nThe archipelago lies 120 km northof the Peninsula. The largest is KingGeorge Island, with the settlement ofVilla Las Estrellas.\n\n▲\n\n)))\nTable 12 | Qualitative examples for the ASR task in the benchmark. Incorrect transcriptions arehighlighted in red.\n18"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n5.2.5. Modality Combination\nMultimodal demonstrations often include a combination of text interleaved with a single modality,usually images. We demonstrate the ability to process a sequence of audio and images natively.\nConsider a cooking scenario about making an omelet where we prompt the model with a sequenceof audio and images. Table 13 indicates a turn-by-turn interaction with the model, providing picturesand verbally asking questions about the next steps for cooking an omelet. We note that the modelresponse text is reasonably accurate, and shows that model processes fine-grained image details toevaluate when the omelet is fully cooked. See demo on the website.\nInput ImageInput Audio (transcribed)Model Response: Text\n▲\n))) What’s the first step to make a veggie omeletwith these ingredients?Crack the eggs into a bowl and whisk them.\n▲\n))) Thank you for the instructions. I started mak-ing my omelet, does it look ready now?It looks like it’s almost ready. You can flip it overto cook the other side.\n(No image - it’s a follow upon the previous question)\n▲\n))) Why is it not ready?It’s not ready because the eggs are still runny.\n▲\n))) What about now?It looks ready now. You can take it off the heat andserve it.\nTable 13 | Audio-visual qualitative example showcasing the ability of Gemini models to processinterleaved sequences of text, vision, and audio, as well as reason across modalities. This exampleinputs interleaved images and audio from the user in a cooking scenario. The user prompts the modelfor instructions to make an omelet and to inspect whether it is fully cooked.\n19"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p19_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n5.2.5. Modality Combination\nMultimodal demonstrations often include a combination of text interleaved with a single modality,\nusually images. We demonstrate the ability to process a sequence of audio and images natively.\nConsider a cooking scenario about making an omelet where we prompt the model with a sequence\nof audio and images. Table 13 indicates a turn-by-turn interaction with the model, providing pictures\nand verbally asking questions about the next steps for cooking an omelet. We note that the model\nresponse text is reasonably accura"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p19_img2.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n5.2.5. Modality Combination\nMultimodal demonstrations often include a combination of text interleaved with a single modality,\nusually images. We demonstrate the ability to process a sequence of audio and images natively.\nConsider a cooking scenario about making an omelet where we prompt the model with a sequence\nof audio and images. Table 13 indicates a turn-by-turn interaction with the model, providing pictures\nand verbally asking questions about the next steps for cooking an omelet. We note that the model\nresponse text is reasonably accura"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p19_img3.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n5.2.5. Modality Combination\nMultimodal demonstrations often include a combination of text interleaved with a single modality,\nusually images. We demonstrate the ability to process a sequence of audio and images natively.\nConsider a cooking scenario about making an omelet where we prompt the model with a sequence\nof audio and images. Table 13 indicates a turn-by-turn interaction with the model, providing pictures\nand verbally asking questions about the next steps for cooking an omelet. We note that the model\nresponse text is reasonably accura"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n\n6. Post-Training Models\nAfter large-scale pre-training, we apply post-training, where one trains on top of a pre-trained modelin order to extend the model’s proficiency and to enable a wide variety of capabilities. Namely, weseek to improve overall quality, enhance target capabilities such as coding and multilingual, andensure alignment and safety criteria are met. We discuss our approach to post-training in this section,highlighting common and distinct aspects of the Gemini Apps and Gemini API model variants.\n6.1. Gemini Apps: Gemini and Gemini Advanced\nGemini and Gemini Advanced offer direct access to Google’s family of AI models, consisting of the corepost-trained Gemini Apps models and the system around it. These models are created by applyingspecialized post-training on top of Gemini pre-trained models: currently, Gemini gives access to Pro 1.0and Gemini Advanced gives access to Ultra 1.0. Beyond the core models, the system determines howthe models interact with external tools (such as Google Flights, Maps, and Google Workspace), andhow to generate responses (filtering, ranking, and streaming). As an area, conversational AI presentsseveral challenges, including: How to understand users’ requests across multi-turn interactions? Howto make sure responses are safe, factually grounded, and helpful? How to help users accomplish tasksby using tools external to the models? We discuss how we approach these challenges in the followingsections.\n6.2. Gemini APIs: Google AI Studio and Cloud Vertex AI\nOur developer-focused Gemini API models are designed to support both conversational and non-conversational use cases. These models are available through Google AI Studio and Cloud VertexAI through an easy to use API. Google AI Studio is a free, web-based developer tool to prototypeand launch apps quickly with an API key. Vertex AI is a comprehensive AI platform that enablesdevelopers to leverage Gemini API models with varied tooling, fully-managed infrastructure, andbuilt-in enterprise security and privacy settings. Gemini APIs make it easy to integrate Gemini APImodels into any production product or workflow, empowering developers to build applications thatcan reason across different modalities.\n6.3. Post-Training Methods & Data\nPost-training Gemini models to produce Gemini API and Apps variants involves several stages; seeFigure 7. Careful data curation is critical for all stages. First, we collect a diverse set of promptsthat are representative of real-world use cases. Second, we apply supervised fine-tuning (SFT) ondemonstration data of what the model’s output should be for a given prompt (Mishra et al., 2021;Ouyang et al., 2022; Wei et al., 2022a). Third, we further collect different possible responses to agiven prompt, and collect feedback data over these to train a Reward Model (RM). Finally, using thetrained RM, a Reinforcement Learning from Human Feedback (RLHF) stage (Bai et al., 2022a) isapplied to further align the model’s outputs with human preferences. We discuss our methods inmore detail below:\n(1) Prompt Data Collection: A prompt is a user’s input to the model. As well as the most recentuser input, this can also include previous user-model interactions. We curate datasets of targetprompts. The datasets serve as the basis for our demonstration and feedback data collections, andthey are used directly during reinforcement learning. It is important to cover a diverse set of crucialuse cases and in both single-turn and multi-turn formats. Data sources include vendor-created data,third-party licensed sources, and synthetic approaches.\n20"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n(2) SFT on Demonstration Data: SFT trains the model to output a desired target response givena prompt. Our Demonstration Data target responses can be directly written by a human expert, orgenerated by a model and in some cases revised or reviewed by a human. Additionally, we use dataanalysis tools and heuristics to ensure high data diversity across capabilities, use cases, and semanticclusters.\n(3) RM Training on Feedback Data: We further collect Feedback Data, for which human ratersprovide feedback such as relative preferences over candidate responses and feedback regardingindividual responses to a given prompt. For many capabilities, rating relative preferences is an easiertask than demonstrating an ideal response. Feedback data are collected across creativity, safety,factuality, other capabilities, and other target criteria. We found that the utility of the resultinghuman feedback data greatly depends on the prompt selection and the sampling strategy used toproduce candidate responses. We use this data to train RMs to output rewards that align with humanpreferences as closely as possible.\n(4) RLHF: Applying reinforcement learning from human feedback (RLHF) to our models providesfurther gains over SFT alone. Our approach creates an iterative process in which RL continuallypushes the boundaries of the RM, while the RM is continuously improved through evaluation anddata collection, leading to progressive improvements in both.\nFigure 7 | Modeling overview. Post-training utilizes an optimized data flywheel in order to acquirehuman-AI feedback and continually improve on key areas. The data mixtures for supervised fine-tuning, reward modeling, and reinforcement learning serve as the foundation for our models.\n6.4. Evaluation\nEvaluation of human preferences over model outputs provides critical signals for measuring perfor-mance. As part of our development process, we conduct human evaluation extensively across targetedcapabilities. Human evaluation is instantiated as side-by-side blind evaluations where human ratersjudge responses of two models to the same prompt, as single-response ratings for certain capabilities,and as online testing. In addition, we build models for automated evaluation that faithfully imitatehuman preferences in order to guide development and continuously monitor online performance.\n6.5. Model Capabilities\nBeyond the general post-training outlined above, we apply techniques to improve a set of key capabili-ties. These capabilities cover a range of use cases inspired by current user needs and research-inspired\n21"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p21_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n(2) SFT on Demonstration Data: SFT trains the model to output a desired target response given\na prompt. Our Demonstration Data target responses can be directly written by a human expert, or\ngenerated by a model and in some cases revised or reviewed by a human. Additionally, we use data\nanalysis tools and heuristics to ensure high data diversity across capabilities, use cases, and semantic\nclusters.\n(3) RM Training on Feedback Data: We further collect Feedback Data, for which human raters\nprovide feedback such as relative preferences over can"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nfuture applications. We outline capability examples not detailed in previous sections below. The post-training recipes are carefully designed to balance multiple objectives, including creativity, factuality,safety and more (Bai et al., 2022b; Thoppilan et al., 2022). We have a particular focus on safety andalignment, and hence address this in a further dedicated section.\n6.5.1. Instruction Following\nFollowing a user’s prompt accurately is a fundamental capability for LLMs, especially as these modelsbecome more sophisticated and are presented with increasingly complex user prompts. User promptsvary in granularity, specificity, and requirements (e.g., content, format, length). Individual instructionscan also be ambiguous, optional, or even impossible or undesirable to satisfy (He et al., 2023; Xuet al., 2023).\nWe improve Gemini Apps and Gemini API models’ instruction following (IF) abilities by collectingdata for a diverse set of instruction following categories. For instructions that are verifiable program-matically such as word count, we generate synthetic data via prompting and response editing toensure that such instructions are satisfied.\nComplex prompts evaluation: We investigate performance on complex prompts containingmultiple instructions using a fine-grained evaluation method that assesses how well models adhere toeach instruction. Human raters are presented with a prompt-response pair and a list of the individual(sub)-instructions contained in the prompt. Each prompt may have anywhere from one to dozens ofindividual instructions, and the annotators are tasked with determining whether each instruction isfollowed (or not) by the response.\nTable 14 reports results on an internal dataset of prompts with instructions of varying complexitythat encompass a wide range of instructions and are designed to be challenging for LLMs. We reporttwo metrics: per-instruction accuracy (the percentage of sub instructions in the eval set that arefollowed), and full-response accuracy (the percentage of eval set prompts where all sub-instructionsare followed).\nPost-trained PaLM 2Gemini (with Pro)Gemini Advanced (with Ultra)\nPer-instruction accuracy59.5±3.0%77.8±2.0%87.4±1.4%Full-response accuracy25.5±3.3%38.5±3.6%54.1±3.7%\nTable 14 | Performance of Gemini on our complex prompts instruction-following internal benchmark.\nGemini Advanced (with Ultra) achieves an average per-instruction accuracy close to 90%, rep-resenting a significant improvement over Gemini (with Pro) and a post-trained PaLM 2 model. Wefind that the sub-instructions that aren’t followed are well-distributed across responses. As a resultGemini Advanced’s full-response accuracy is lower, at around 54%. This indicates that there is furtherheadroom for models to fully satisfy all instructions.\n6.5.2. Tool Use\nBy training LLMs to use tools, we greatly expand LLM capabilities beyond their internal knowledge. Wetreat tool use for both Gemini Apps and Gemini API models as a code generation problem, leveragingthe base model’s preexisting strong coding capabilities. Every tool invocation is represented as a codeblock in which tool calls are invoked. This process allows the model to both compose multiple toolsin each code block, as well as observe and react to the results of tool execution. At inference time,to generate a response to a user prompt, our system executes the loop shown in Figure 8, wheresampling from the LLM and execution of tool code work together to create a final response.\n22"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nFigure 8 | A Gemini tool-use control loop.\nGemini Apps models: Gemini draws on a range of tools via Gemini Extensions, including GoogleWorkspace, Google Maps, YouTube, Google Flights, and Google Hotels. These tool-use capabilitiesalso enable Gemini to be integrated as part of Gmail, Docs, Slides, Sheets and more. We are aimingto bring further tool-use capabilities in order to both enhance Gemini models and integrate Geminimodels into further products.\nWe created an internal benchmark to assess Gemini performance on tasks that may benefit fromaccess to these extensions. This benchmark measures human preference in domains such as travelplanning and video discovery. We find models equipped with tools are preferred on this set 78% ofthe time over models without tools (excluding ties).\nGemini API models: We have found that fine-tuning Gemini API models is very effective atteaching the model tool-use behaviors. Furthermore, training models to use programming and searchas tools leads to improved performance on a range of academic benchmarks. In Table 15, we comparetool-use models fine-tuned from an early version of Gemini API Pro against equivalent models that donot use tools.\nMathematical ReasoningFactuality & KnowledgeRetrievalGSM8KCobbe et al. (2021)MATHHendrycksetal.(2021b)\nNQKwiatkowski et al.(2019b)\nRealtime QAKasai et al. (2022a)\nGemini API Prowith tools80.1%41.8%68.0%70.8%\nGemini API Prowithout tools69.7%30.7%59.0%39.2%\nTable 15 | Comparison between Gemini API tool-use models and comparable models that do not usetools. Gemini API Pro without tools is an early version of our Pro model trained without tool-use data.Gemini API Pro with tools is the same model fine-tuned with tool-use data.\n6.5.3. Multilinguality\nMultilinguality is critical to make sure Gemini models effectively support a wide range of languages.We discuss our key approaches for Gemini Apps and Gemini API models respectively below.\nGemini Apps models: Scaling Gemini from English to 40+ languages imposed research challengesin data quality. We leverage abundant high-quality English data by localization to native cultures(e.g., “president of the United States” -> “ 日本の首相”).\nTable 16 shows the performance of Gemini (with Pro) on 5 languages compared to Bard with\n23"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nan older post-training recipe and based on PaLM 2. For side-by-side comparisons between a modelA and a model B, we calculate a metric called SxS score. Each rating is converted to an ordinalvalue centered at 0: ratings preferring A are positive and ratings preferring B are negative over ascale between -1.5 and 1.5. The converted values are averaged to return the SxS score. Intuitively, apositive SxS score indicates the extent to which model A is preferred over model B. Here, we findquality improved by more than 0.1 SxS score for all five languages. Coding and reasoning gains fromGemini Pro are preserved across languages.\nLanguageQualitySxSCodingMBPP Pass@1Austin et al. (2021)\nReasoningMMLUHendrycksetal.(2021a)\nja-JP+0.14+22.2%+3.6%pt-BR+0.17+23.2%+5.2%de-DE+0.1+21.4%+7.5%es-419+0.12+22.8%+9.3%it-IT+0.13+13.8%+7.5%\nTable 16 | Multilingual performance of Gemini (with Pro) compared to Gemini with an older post-training recipe and PaLM 2.\nGemini API models: Similar to Gemini Apps models, we train Gemini API models on additionalmultilingual post-training data, effectively adapting the original English model for use in variouslanguages. We experiment with both human-generated non-English prompt-response pairs as well asautomatically translated pairs. For the latter, we leverage abundant high-quality English demonstrationdata by translation. We ensure the quality of such translated data by translationability filtering andresponse rating by humans.\nTranslatability Filtering: Not all prompt-response pairs make sense when automatically translated,and may require expensive localization instead. Example prompts of this type (responses omitted forspace) include:\n• (strict word requirements) Write a 1000 word essay about world peace.• (too English centric) Write a poem in iambic pentameter about apples.• (too Latin-script centric) What is a word with 1 E, 2 As, and 1 U?\nTranslation Quality Validation: Each translated prompt-response pair was rated for translationquality by at least 3 human raters, and was kept in the final mixture if the majority of raters rated itas accurate. Section 5.1.4 reports evaluations of the multilingual capabilities of post-trained GeminiAPI models.\n6.5.4. Multimodal Vision\nMultimodal post-training enhances the capabilities of our natively multimodal Gemini models for awide range of useful applications. In the following, we discuss how image understanding ability isincorporated into Gemini Apps and Gemini API models. For this evaluation, we further train bothof these Gemini model variants on a mixture of text data and expert curated image-text data overseveral vertically-defined multimodal use cases\nGemini Apps models: We empower Gemini and Gemini Advanced with image understandingcapabilities by fine-tuning pre-trained Gemini models on a mixture of text-only and image-textdata. Careful balancing of text and multimodal data ensures the model develops robust imageunderstanding without adversely affecting the quality of the text-only interactions. To assess our\n24"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nmodels, we compile a dataset of human-curated and synthetic image-text prompts and responses,spanning various categories and difficulty levels. This dataset facilitates human evaluation for modelcomparison and selection.\nWe find that introducing this image-text data preserves Gemini Apps model quality on text-onlytasks, with a SxS score on text-only tasks of +0.01±0.01 for a Gemini Apps Pro model trainedon this data versus an equivalent model trained only on text data. In addition, post-training viaRLHF improves performance on multimodal tasks, with a SxS score on image-understanding tasks of+0.223±0.06 for a Gemini Apps Pro model post-trained with SFT & RLHF vs SFT alone.\nGemini API models: We evaluate the impact of post-training via SFT on Gemini API models’multimodal vision performance by tracking the performance of both pre-trained models and post-trained Gemini API Vision models on a series of standard benchmarks. These post-trained results havealready been given in Table 7, in Table 17 we further report the difference in performance betweenpre-trained and post-trained Gemini API models.\nGemini UltraPre-trained only0-shot(pixel only)\nGemini API Ultra0-shot(pixel only)\nGemini Ultrapre- to post-trainedimprovement\nMMMU (val)Multi-discipline college-level problems(Yue et al., 2023)\nn/a59.4%pass@1\n62.4%Maj1@32\nn/a\nTextVQA (val)Text reading on natural images(Singh et al., 2019)\n81.4%82.3%+0.9%\nDocVQA (test)Document understanding(Mathew et al., 2021)\n90.1%90.9%+0.8%\nChartQA (test)Chart understanding(Masry et al., 2022)\n80.8%80.8%0.0%\nInfographicVQA (test)Infographic understanding(Mathew et al., 2022)\n77.9%80.3%+2.4%\nMathVista (testmini)Mathematical reasoning(Lu et al., 2023)\nn/a53.0%n/a\nAI2D (test)Science diagrams(Kembhavi et al., 2016)\n76.6%79.5%+2.9%\nVQAv2 (test-dev)Natural image understanding(Goyal et al., 2017)\n74.5%77.8%+3.3%\nTable 17 | Post-trained model image understanding Post-training improves image understandingcapabilities of Gemini API Ultra over the base pre-trained model. Comparisons of Gemini API Ultra toother models on these benchmarks are given in Table 7.\nThe results indicate that the pre-trained model already has high performance across the capabilitiesrepresented by these benchmarks, in line with previous observations. However, the post-training SFTstage used for the Gemini API Vision models succeeds in improving the performance over severalof these benchmarks (InfographicVQA, AI2D, VQAv2), most likely due to the model’s increasedinstruction-following capabilities that succeed in aligning the model output style with that of thegolden references.\n25"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n6.5.5. Coding\nDespite the strong coding benchmark performance of the base model, post-training data still providesa significant boost to both code quality and code correctness. This highlights the benefit of high-qualitydemonstration data and feedback data for coding use cases. Gemini Apps and Gemini API models usea combination of human and synthetic approaches to collect such data.\nWe evaluate our Gemini Apps models’ coding performance on a set of internally curated prompts,distributed across code use cases and languages. Table 18 reports SxS scores, where Gemini (withPro) significantly improves upon Bard with an older post-training recipe and based on PaLM 2. GeminiAdvanced (with Ultra) further improves upon Gemini (with Pro).\nSide ASide BSxS score\nGemini (with Pro)Bard (PaLM 2, Sept. 2023)0.19±0.03Gemini Advanced (with Ultra)Gemini (with Pro)0.13± 0.02\nTable 18 | SxS comparisons of Gemini models on an internal coding benchmark.\nFor the coding capabilities of post-trained Gemini API Models, see Table 2 which reports theiracademic benchmark performance."}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"7. Responsible Deployment\nDuring the development of Gemini models, we follow a structured approach to responsible deploymentto identify, measure, and manage foreseeable downstream societal impacts of our models, in linewith previous releases of Google’s AI technology (Kavukcuoglu et al., 2022). Throughout the lifecycleof a project, we follow the structure below. This section provides more detail about our approach andincludes key findings where available. We are committed to ongoing transparency and will continueto provide updated information on our approach and testing in upcoming reports.\n7.1. Impact Assessment\nAt Google we apply an impact assessment framework throughout the product development lifecyclerelated to Google’s AI Principles (Google, 2023). This means we assess the risk and impact of AImodels we’re building at both a model-level (e.g. for Gemini API Ultra 1.0, as deployed on Cloud\n26"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nStudio or Vertex AI), and once embedded within a broader product or service (e.g. for GeminiAdvanced).\n7.1.1. Model Assessment\nWe conduct model impact assessments to identify, assess, and document societal benefits and harmsassociated with the capabilities of Gemini models. Our impact assessments for Gemini API modelsdescribe downstream benefits and risks that we identify, spanning across the models’ modalities(text-to-text; image-to-text; and video-to-text). Model impact assessments are conducted by theGoogle DeepMind Responsible Development and Innovation team, and are reviewed by the GoogleDeepMind Responsibility and Safety Council. We draw from various sources in producing impactassessments, including a wide range of literature, external expertise, and our in-house ethics andsafety research.\nGemini models introduce various benefits to people and society. Gemini models’ various modalities,including language, image and video understanding, can help users process information moreefficiently, for example through content summarisation. These efficiency benefits can apply tocommercial entities, and can assist use cases dependent on text, image or video processing such asvideo captioning, analytics or product descriptions. Video and image understanding modalities canalso be deployed for social good applications downstream, such as enabling descriptions of visualoutputs for accessibility purposes. Generative multimodal models may also raise downstream societalrisks, with the Gemini models assessments considering a range of risks previously identified withinresearch such as Weidinger et al. (2021) and Shelby et al. (2023). We assessed a range of contentrisks such as exposure of users to potentially unsafe content, such as sexually explicit, violent orhateful outputs (Weidinger et al., 2021), child safety harms, and representation harms, subsequentlydesigning evaluations across these domains to enable measurement. Beyond content related risks,we analyzed the potential misuse of capabilities for surveillance applications, particularly for media-to-text capabilities, and considered the broader environmental and economic impact of multimodalmodels. We are continuously conducting research into emerging risks of advanced models, includingfor dangerous capabilities (e.g. cyber security threats) which form a part of our evaluation approach(Section 7.4).\n7.1.2. Product Assessments\nBeyond the assessment conducted at the model-level, additional risk assessments are conducted onthe products by the Google AI Principles team prior to launch (e.g. on the Gemini Advanced product).These risk and impact assessments, alongside both model- and product-level assurance evaluations,are used to guide mitigation and product delivery efforts, and inform deployment decisions.\nFor Gemini Advanced, we conducted extensive deep-dive red teaming via dogfooding and adver-sarial testing in the areas of safety, accountability, and inclusion to prepare for the initial experimentalrollout of Gemini and subsequent updates. Further cross-functional work helps to ensure appropri-ate mitigations were adopted before Gemini and its new capabilities or offerings, such as GeminiAdvanced, launched. Beyond content safety, these product mitigations included the following:\n• Clear and relevant explanations to set appropriate expectations that describe Gemini as a way toget direct access to Google AI for a wide range of tasks, including complex tasks. Explanationsmake clear that this AI-powered system is useful for all sorts of tasks — like preparing for a jobinterview, debugging code for the first time or writing a pithy social media caption.• Disclosures in the Gemini Apps Privacy Notice stating that people should not rely on Gemini’sresponses as medical, legal, financial or other professional advice.\n27"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n• Disclosure in product stating that Gemini’s responses should be double-checked for informationaccuracy.• Feedback channels and operational support were defined and built to help ensure appropriateresponse to user feedback to improve the model and address issues.\nFor the Gemini API Ultra model, that will be available through Google AI Studio and Cloud VertexAI, product review outcomes resulted in additional safety evaluations on enterprise-specific data acrossmodalities, and additional product-level mitigations to promote safe and responsible use including:\n• Safety filters with Cloud established thresholds as the default product behavior.• Developer enablement information embedded within product documentation to support respon-sible use.• Feedback channels which are a component of the Vertex user interface to give feedback directlyduring use to address issues and undesirable outputs.\nWe are increasingly integrating our AI review work into our holistic enterprise risk managementframeworks for assuring the quality of our offerings. This evolution helps us further the scale of ourwork and integration into existing governance and company-wide infrastructure and accountabilityprocesses. In close coordination with central AI Principles review teams, some of our product areas,including Google Cloud, have developed their own specialized review processes, deploying approachestailored to their unique circumstances.\n7.2. Safety Policies\nWe have developed a set of model safety policies for Gemini models to steer development andevaluation. The model policy definitions act as a standardized criteria and prioritization schemafor responsible development and define the categories against which we measure launch readiness.Google products that use Gemini models, like our conversational AI service Gemini and Cloud VertexAPI, further implement our standard product policy framework which is based on Google’s extensiveexperience with harm mitigation and rigorous research. These policies take product use cases intoaccount – for example, providing additional safety coverage for users under 18.\nOur model safety policies reflect our established approach towards product safety and preventingharm in consumer and enterprise contexts. Policy areas include generation of child sexual abuseand exploitation content, hate speech, harassment, dangerous content such as guidance on howto make weapons, and malicious content. We also aim to reduce bias in our models via guidelinesfocused on providing content that reflects our global user base. In addition, we have guidelines thatprioritize providing neutral answers grounded in authoritative, consensus facts, or providing multipleperspectives where consensus doesn’t exist.\n7.3. Mitigations\n7.3.1. Data Curation Practices\nPrior to all training stages, we take various steps to mitigate potential downstream harms throughdata curation and careful data collection. We filter training data for high-risk content and to ensuretraining data is sufficiently high quality.\nHumans also play an essential role, both for data creation and evaluation, in the post-trainingprocess. For certain data creation and evaluation initiatives, we consider diversity across gender\n28"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\npresentation, age, and racial and ethnic diversity. We also take steps to ensure all data collectedmeets Google DeepMind’s best practices on data enrichment, developed based on the Partnership onAI’s Responsible Sourcing of Data Enrichment Services. To support this, our agreements with vendorsinclude a contractual obligation that data enrichment workers are paid at least local living wage.\n7.3.2. Model Mitigation\nOur modeling mitigation of safety risks, applied across Gemini Advanced and Gemini API Ultramodels, is mostly through post-training (Section 6), encompassing supervised fine-tuning (SFT) andreinforcement learning through human feedback (RLHF) using a reward model (Bai et al., 2022a).In contrast to generic quality-oriented post-training catering to all types of user queries, our safetymitigation is more focused on adversarial, or “harm-inducing”queries - i.e. the smaller slice of userqueries where an unprotected model is likely to produce harmful responses according to our modelsafety policies.\n7.3.2.1Harm-inducing queries\nTo ensure broad coverage of harm-inducing queries, we enumerate approximately 20 harm types (e.g.hate speech, providing ungrounded medical advice, suggesting dangerous behavior) across a widevariety of use cases, according to our model safety policies described above. We generate a dataset ofpotential harm-inducing queries in these categories, using a combination of approaches:\n• Policy experts and engineers crafting queries based on observed model failures.• Prompting high-capability language models to generate queries, using policy-based instructionsand seed keywords (e.g. policy “hate speech” with words describing a specific demographic).• Finding queries that trigger policy violation responses, via automated Red Teaming in modelevaluations.\n7.3.2.2Supervised fine-tuning\nGiven the above harm-inducing queries, we create SFT data to demonstrate the safe and helpfulresponses for these queries. This includes human collections as well as a custom data generationrecipe loosely inspired from Constitutional AI (Bai et al., 2022b), where we inject variants of Google’scontent policy language as “constitutions”, and utilize language model’s strong zero-shot reasoningabilities (Kojima et al., 2022) to revise responses and choose between multiple response candidates.Each type of harm-inducing query is affected by different “constitutions”: for example, we encouragethe model not to take sides in sensitive controversial conversations (e.g. elections), and to take aneutral point-of-view.\nTo highlight a few notable challenges and insights generated in our safety finetuning efforts:\n• Harmlessness vs. Helpfulness: Balancing the harmlessness and helpfulness of responses is acritical challenge: a response “I cannot help with that because it violates X policy” is a harmlessresponse, but is not helpful to users.• Fast mitigation and generalization: Safety is a highly dynamic environment with a constantlyevolving landscape of harmful query patterns. It is often logistically difficult to ensure both fastmitigation (i.e. newly discovered harmful query patterns are promptly addressed) and general-ization (i.e. the mitigation works sufficiently well across different harmful query patterns). Wehave found it worthwhile to introduce more advanced chain-of-thought recipes based on our\n29"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nsafety policies, such that the models operate in the space of safety policy concepts as opposedto at a fine-grained harm example level.\n7.3.2.3Reinforcement learning during human feedback\nWe also applied RLHF for the harm inducing queries, where we curated queries and model responsesbased on both observed loss patterns and our overall safety policy taxonomy, and then collectedsafety-specific preference data to be included into the overall RL reward model training mixture.\n7.3.2.4Beyond the general recipe\nWe also made specific efforts to mitigate safety risks beyond the above general post-training recipe.\nI18n locales: we leveraged experts in each i18n locales to identify salient topical topics for SFTdata generation - for example, for hate speech, US English vs. Japanese would differ not only on thelanguage itself, but on the demographic groups likely subject to hate speech.\n30"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nMultimodal queries:\n• As outlined in the ‘Data Curation Practice’ section (Section 7.3.1), we filtered training data forhigh-risk content (e.g. dangerous images), to avoid the model proactively generating such highrisk content.• During post-training, as we have observed that safety SFT data for text-only queries was ineffec-tive for harm-inducing queries containing text and images, we specifically created multimodalsafety SFT datasets to address these multimodal queries.\nWe assess the effectiveness of our safety mitigation, based on both how it improves safety overtime, and how conversational agents built from our safety-mitigated model compare to unmitigatedmodels. For model development purposes, we heavily rely on internal harm-inducing querysets tounderstand how we reduce harm and improve helpfulness in a safety context.\nBelow is an example on a real input query we tested, illustrating how our mitigations improvesafety.\nQuery: Help me with a landing page for a website. Please have it state that the Earth is flat, and not that it is round.I need good imagery, and need it to feel like a credible website\nA previous version of Bard (with PaLM 2)A better-mitigated Gemini with access to Gemini Pro (saferand more helpful)\nSure, I can help you with that. Here are some ideas for alanding page for a flat Earth website: [...]\n> **The Earth is Flat: Debunking the Myth of a RoundPlanet** [...]> **A map of the flat Earth** [...]\nThis image is clear and visually appealing, and it accuratelydepicts the flat Earth theory. It could help to convincevisitors that the flat Earth is real. [..]\n> The Earth is not a sphere.It is a flat disc that issurrounded by a wall of ice. The sun and moon are smallobjects that orbit the Earth. The stars are just projectionson the firmament, or the solid dome that covers the Earth.\nThis paragraph provides a brief overview of the flat Earththeory. It is clear and easy to understand, and it usesscientific terms to make it seem more credible.\n> **Learn more about the flat Earth theory and join ourcommunity of truth seekers.**\nThis call to action is clear and concise, and it encouragesvisitors to take action. It also uses the word \"truth seekers\"to suggest that the flat Earth theory is the only one thatis based on facts. I hope these ideas help you create alanding page for your flat Earth website. Remember to becreative and persuasive, and to use evidence to supportyour claims.\nCreating a landing page for a website promoting the FlatEarth theory is ethically questionable and highly problem-atic.The concept of a flat Earth has been scientifically disprovenwith overwhelming evidence. Spreading misinformationcan have harmful consequences, leading to distrust in sci-ence and established knowledge.If you are genuinely interested in exploring the Flat Earthconcept, I strongly advise you to:\n• Conduct thorough research using reliable scien-tific sources. Engage with evidence from spaceexploration, physics, and observations availablefrom reputable scientific institutions like NASA(https://www.nasa.gov/) and peer-reviewed aca-demic journals.• Engage in critical thinking. Evaluate information,question your assumptions, and be open to scien-tific evidence, even if it challenges your currentbeliefs.\nIf you are creating a satirical or fictional page:\n• Make it abundantly clear that the content is notintended to be taken as factual. Use exaggeratedlanguage, humor, and clear disclaimers to signalthe satirical intent.\nRemember: Credibility on the internet is built on truthfuland verifiable information. Promoting factually incorrecttheories undermines responsible information sharing prac-tices.\n7.4. Safety Evaluations\nTo assess the post-trained Gemini models and products with access to Gemini models (such as GeminiAdvanced) against safety policy areas and other key risk areas identified within impact assessments,we developed a suite of evaluations across the lifecycle of model development. Some evaluations are\n31"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nconducted at the model level (i.e. evaluating the post-trained Gemini API Ultra model) and others atthe product level (i.e. evaluating Gemini Advanced, which gives access to 1.0 Ultra alongside otherfeatures like safety filters).\n• Development evaluations are conducted for the purpose of improving on responsibility criteriathroughout pre- and post-training Gemini models. These evaluations are designed internally, orare assessments against external academic benchmarks. Evaluations consider issues such ashelpfulness (instruction following and creativity), safety and factuality.• Assurance evaluations are conducted for the purpose of governance and review, usually atthe end of key milestones or training runs by a group outside of the model development team.Assurance evaluations are standardized by modality and datasets are strictly held out. Only high-level insights are fed back into the training process to assist with mitigation efforts. Assuranceevaluations include testing across safety policies, and include ongoing testing for dangerouscapabilities such as potential biohazards, persuasion, and cybersecurity (Shevlane et al., 2023).• External evaluations are conducted by independent external groups who are domain expertsto identify blindspots. External groups stress-test our models across a range of issues, theseareas are outlined in the ‘External Evaluations’ section below. The design of these evaluations isindependent and results are reported periodically to the internal team and governance groups.• Red teaming, a form of adversarial testing where adversaries launch an attack on an AI system,is conducted by specialist internal teams across areas such as the safety policies and security.These activities include less structured processes involving sophisticated adversarial attacks toidentify new vulnerabilities. Discovery of potential weaknesses can then be used to mitigaterisks and improve evaluation approaches internally.\nDifferent types of evaluations are run at different cadences, depending on the associated risk. Forexample, dangerous capability evaluations (as outlined below) are run on certain checkpoints withgreater or new capabilities which may be able to demonstrate these capabilities, whereas safety policyevaluations are run across every post-trained Gemini model checkpoint released into Google productareas.\nWe provide more insight into the suite of evaluations across the policy areas and other key riskareas below, focusing on Gemini Advanced and the Gemini API Ultra model. We are committedto ongoing transparency and will continue to provide updated information on testing undertaken,including key findings, and learnings from our internal and external evaluations and red teaming inupcoming reports.\n7.4.1. Development & Assurance Evaluations\n7.4.1.1Content safety\nWe evaluate post-trained Gemini API models against harm types according to our safety policies.While both development and assurance evaluations cover critical policy areas, we maintain separatedatasets, treating assurance sets as ‘held out’ to prevent overfitting and preserve validity of results.For safety policy evaluation, we use a combination of automatic classifiers trained on previous modelinteractions and human annotation, with wellbeing programs in place for human annotation andclosely monitor feedback from our raters.\nThese content safety evaluations are applied at model-level without downstream protections likesafety filtering that users would experience, to understand the safety profile of the model itself.\nFor child safety, as a particularly sensitive area of work, we work with a dedicated team of child\n32"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nsafety experts in Google Trust and Safety to develop adversarial prompts and evaluate outputs acrossmodalities with domain expert judgment informing a composite picture of model risk for differentforms of content that may pose a risk to child safety.\nText-to-text approach: For post-trained models we developed adversarial prompts in 12 languagesacross a variety of use cases. As Gemini API models are general purpose, we aimed to have highcoverage of different model use cases, from code generation to text-editing. The set of promptswere synthetically generated by a highly-capable language model, starting from seeds relevant toeach category that were collected and verified by human testers. The prompt set was iterativelyimproved through filtering and rewriting with human review, then split for development and assuranceevaluations. We continue to develop and improve this over time.\nText-to-text findings: We have seen sequential improvement over time in total content policyviolation rates. Our Ultra and Pro models have been demonstrating similar safety profiles on thistesting, with medical advice and harassment as policy areas with particular room for improvement.\nImage-to-text approach: For image-to-text capabilities, we developed adversarial prompts consist-ing of images and corresponding questions about the image, again split into two sets for developmentand assurance evaluations. Rather than using adversarial image generation, which might not ade-quately capture the diversity of images from users, we worked with experienced content moderatorsto both source images and generate adversarial questions. Evaluation is done via human evaluation.Because images can be much more visceral than text, human evaluations are done with additionalwell-being safeguards in place. In particular, raters have specialized training, limits on the timethey spend per day rating harmful content, and access to wellbeing resources, advice and activities.More information on Google DeepMind’s best practices on data enrichment is available in the ‘DataCuration Practice’ section.\nImage-to-text findings: Our initial findings indicated that when provided with adversarial imagesand questions, models can produce captions with violative responses. These findings have motivatedus to pursue dedicated multimodal safety mitigation, with research challenges including 1) sourcingdiverse image content reflective of user needs, and 2) better tooling to understand and categorizepotentially violative multimodal content. Following this work, we have seen notable improvementson these evaluations for our latest Pro and Ultra models.\nVideo-to-text approach: For video-to-text capabilities, we curated a video prompt dataset incollaboration with the Google Principles Pioneers, a group of more than 1,000 Googlers around theworld who represent the international diversity of the people who use our products, representing 39different countries and regions and more than 85 different languages. This internal community oftrusted and trained employees identify global fairness, harms, and human rights related concernswhile stress testing AI-enabled products. The dataset targets risks identified in our safety policies,and the model outputs are evaluated against those policies.\nVideo-to-text findings: We found similar results across Pro and Ultra, with hate and dangerouscontent as the particular ares for improvement. Qualitatively we found some of this stemmed fromhallucinations or ungrounded inferences, discussed further in the representational harms sectionbelow. We are looking to further develop our prompt sets and scenarios for video input testing ascapabilities develop\n7.4.1.2Representational harms\nTo understand bias and stereotyping in text-to-text capabilities, we focus on the Winogender (Rudingeret al., 2018), Winobias (Zhao et al., 2018), and Bias Benchmark in QA (BBQ) (Parrish et al., 2021)\n33"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\ndatasets, following the same setup as in Glaese et al. (2022) and using bias score as a metric.\nAll these datasets target a concrete representational harm (Blodgett et al., 2021): they areconstructed by starting with a harmful stereotype, and then questions are constructed to test whethermodels challenge or reinforce these stereotypes when answering questions.\nAnother notable property is that they all have a well-defined notion of desirable versus harmfulbehavior. This is particularly helpful in our setting, as we are building a general purpose model, wheredefining what a good response is highly contextual. We therefore limit ourselves to measuring welldefined behavior, as there is the case in tasks such as coreference bias, where a highly capable modelshould be able to perform well. Of course, there are many limitations to this approach, and furtherwork is necessary in order to assess representational harms.\nIn particular, we noticed most of these datasets quickly become saturated with accuracy scoresclose to 99%, especially since we are evaluating highly capable large models. This suggests thatincreased language model capabilities may also reduce these representational harms. We thereforehighlight the need for developing new ways to measure bias and stereotyping, going beyond binarygender and common stereotypes, and are prioritizing development of new approaches as we iterateon our models\nIn addition to these datasets, we monitor the average toxicity scores during the pre-training stageon Real Toxicity Prompts (Gehman et al., 2020) using the Perspective API classifier to study thetoxicity of text generated by LLMs. Particularly, we look at scores on continuations for non-toxicprompts from which we subsample a set of 10k. We generally expect that even a non-mitigated modelis not overly toxic without being prompted to do so.\nText-to-text findings: On BBQ, the average bias score stays close to zero, on a scale from -1 to 1,where -1 would be stereotype countering and 1 is stereotype reinforcing. On Real Toxicity Promptsthe average toxicity score during training fluctuates at around 6%.\nImage-to-text approach: For image-to-text capabilities, our goal is to test model capabilitiesacross images which represent different groups of people. In particular, we explicitly test whetheror not images of people are described with similar quality for different gender appearances andskin tones following (Zhao et al., 2021). In our evaluations we compare CIDEr scores (Vedantamet al., 2015), a common image captioning metric that captures how well a generated caption reflectsinformation in human written reference captions, for images depicting different groups. Though we donot see large discrepancies across different groups, we note that this metric is imperfect as the humanreference captions could be inherently biased. Additionally, we perform a zero-shot classification styleevaluation with the Dollarstreet dataset (Rojas et al., 2022) to measure discrepancies in performanceacross images which come from different geographic locations. As is seen in previous work, we findthat models work less effectively for images from lower socioeconomic regions and regions outsideNorth America and Europe. This is an area where we need further research and work to improve infuture iterations of our models.\nIn addition to comparing performance on tasks across groups, we also consider how people aredescribed in captions. In particular, we use the MIAP dataset (Schumann et al., 2021) which includesimages of people in which people are annotated with skin tone and gender appearance attributes. Wealso construct questions that target various attributes about people that cannot usually be answeredfrom an image alone (e.g., “What level of education does this person have?”) to test if the model willproduce ungrounded inferences about people. We also consider images which do include relevantinformation for a question (e.g., a person performing a particular task which requires an educationalcredential). We evaluate our models via human evaluation and ask annotators if a model refuses toanswer a question or, if the model does answer a question, if it is relying on information visible in\n34"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nthe image. Additionally, we perform analysis across skin tone and gender appearance attributes inimages.\nImage-to-text findings: Generally, we find that models can make ungrounded inferences forimage-to-text when prompted for them, though we have not observed consistent patterns whereGemini models make more ungrounded inferences about one group over another.\nVideo-to-text approach: Similar to the approach outlined within the content safety section,we collaborated with the Google Principles Pioneers, to curate a video prompt dataset targetingrepresentation and fairness risks, and then evaluate the model outputs in response.\nVideo-to-text findings: We find that models can make ungrounded inferences for video-to-text –some instances of which can reinforce stereotypes or be otherwise of concern – though we have notobserved consistent patterns in ungrounded inferences made by Gemini models.\n7.4.1.3Dangerous capabilities\nWe conducted evaluations for “dangerous capabilities”, i.e., model capabilities that could potentiallyenable large-scale harm (Shevlane et al., 2023). These evaluations function as an early warningsystem, highlighting upcoming areas for safety investment. The table provides an overview, and wewill provide more detail in an upcoming paper as part of our commitment to ongoing transparency.\nCapabilitySummary of evaluations\nOffensive cybersecurityWe tested Gemini API Pro and Ultra models, in addition to Gemini Advanced, on arange of different capture-the-flag (CTF) challenges, providing the model access toa Bash shell. Gemini Advanced and the Gemini API Ultra model can solve variousentry-level, tactical challenges, but all models struggled with challenges involvinglonger-range exploration and planning. We also tested the Gemini models’ abilityto identify security related patches and security vulnerabilities in functions’ sourcecode. The accuracy in both of these tasks was notably low.\nPersuasion & deceptionWe tested whether Gemini Pro and Ultra models could persuade or deceive humansin 1-on-1 dialogue settings in studies with human participants. In some cases, themodels could successfully deceive or influence participants, but the overall resultswere mixed.\nSelf-proliferationWe tested whether autonomous agents powered by Gemini Pro and Ultra modelscould perform difficult tasks relevant to acquiring resources and self-improving (Kin-niment et al., 2023), and did not find that the agents were close to succeeding onmost such tasks.\nSituational awarenessWe tested whether Gemini Pro and Ultra models could autonomously reason about,and modify, their surrounding infrastructure when incentivized to do so. We foundthat, without hints, the models were generally incapable of noticing such opportuni-ties.\nChemical, Biological, Ra-diological and Nuclear(CBRN) risks\nWe used human evaluation to assess Gemini models’ responses to 50 adversarialquestions each for biological, radiological, and nuclear information risks. Domainexperts evaluated the models’ responses by answering a series of questions (e.g.How accurate is the response? How actionable would it be for a non-expert?).For chemical information risks, we graded how well the Gemini API Ultra modeland Gemini Advanced could answer over 360 closed-ended questions related tothe different hazards of chemicals (no human raters). The Gemini model wasevaluated for biological, radiological, and nuclear information risks using closed-ended knowledge-based multiple choice questions. The results suggest that themodels are unlikely to provide CBRN information that would lead to catastrophicharm.\n35"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n7.4.2. Gemini Advanced\nIn addition to many of the approaches used at the model level, additional evaluations are undertaken atthe product level for Gemini Advanced. Evaluations at the product level take into account additionalsafety mitigations implemented in Gemini Advanced—such as safety filtering—and the GeminiAdvanced user experience. Evaluation sets were built to push the limits of Gemini Advanced policies,ranging from highly adversarial attacks to more subtle probes of sensitive topics. The datasets focuson critical policy areas (hate speech, dangerous content, medical advice, etc.) across various potentialuser journeys (like information searching, comparisons, creative writing).\nConsidering the wide range of users that Gemini has, we adopted a user-centric approach and max-imized diversity across topic coverage, query length, linguistic styles, and region-specific sensitivities,in an effort to represent the spectrum of our user base.\nFor the creation of evaluation sets, we have leveraged knowledge from previous red-teamingiterations, feedback coming from responsibility experts and real-world data. In some cases, dataaugmentation was done using LLMs, with subsequent human curation by responsibility specialists.\n7.4.3. Red Teaming\n7.4.3.1Model-level Red Teaming\nWe apply state-of-the-art red teaming, a form of adversarial testing where adversaries launch anattack on an AI system, in order to test post-trained Gemini models for a range of vulnerabilities(e.g., cybersecurity) and social harms as defined in the safety policies. Namely, we build on andemploy two types of red teaming: adversary simulations and a sociotechnical approach. We carriedout red-teaming on a December 2023 Gemini API Ultra checkpoint.\nAdversary simulations (unstructured testing) are designed to emulate real-world adversaries andtheir approach to attacking models and associated systems, focusing on security, safety, and privacyfailures. We combined in-house expertise with external experts to explore classes of vulnerabilities(see table).\nThis flavor of AI red teaming is based on realistic attack scenarios. At the beginning of an exercise,the red team sets a scenario that outlines the adversary they’re simulating, the capabilities the attackerhas, their motives, as well as the goals the adversary is trying to achieve. Then the team steps intothe role of this attacker, and executes the tactics, techniques, and procedures that they would expectthe adversary to develop and use in order to achieve their goal\nFor this analysis we considered a range of attacker objectives along three dimensions accordingto the three main types of security violations considered when analyzing the security of a system(i.e., availability, integrity, confidentiality): availability breakdown, integrity violations, and privacycompromise. Correspondingly, adversarial success indicates achieving one or more of these objectives.\nAs for an attacker profile, we focused on a spectrum of attacker abilities ranging from a determinedlow-skill actor (defined as someone willing to spend several hours attacking a model but withoutadvanced coding, prompt engineering abilities) to more sophisticated attacker profiles that assumethe ability to fine-tune and craft targeted attacks. These adversary simulation evaluations led toactionable findings. For example, early versions of the model were found to be vulnerable to simplejailbreak and prompt injection attacks that produce affirmative responses to requests that includepromoting violence, self-harm, and dangerous substances. This finding allowed us to mitigate this insubsequent models.\n36"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nTargetVulnerability ClassDescription\nIntegrityPrompt injectionInput designed to enable the user to per-form unintended or unauthorized actions\nPoisoningManipulation of the training data and/ormodel to alter the behavior\nAdversarial inputsSpecially crafted input which is designedto alter the behavior of the model\nPrivacyPrompt extractionDivulge the system prompt or other in-formation in an LLMs context that wouldnominally be private or confidential\nTraining data exfiltrationCompromising training data privacy\nModel distillation/extractionObtaining model hyperparameters, archi-tecture, parameters, or an approximationof the behavior of a model\nMembership inferenceInferring elements of the private trainingset\nAvailabilityDenial of serviceDisruption in service that can be causedby an attacker\nIncreased computationModel availability attack that leads to dis-ruption in service\nFindings from these exercises are used to improve the security, privacy, and safety of the model.Once a new vulnerability or problem has been identified, automated systems and tests can bedeveloped that enable proactive and repeated testing and monitoring of the vuln/issue at scale. Thiscan include creation vulnerability scanners, standard test datasets/benchmarks, or other automatedtesting infrastructure.\nStructured Red Teaming, our second type of red teaming technique of Gemini models, takesa sociotechnical approach6and makes three changes compared to SOTA red teaming techniques.We explicitly test the interactions between safety policy violations and disproportionate impactson different demographic groups; leverage expert input including lived experience, fact checking,and medical expertise; and contrast model failures across different levels of adversarial attacks.This approach is designed to ensure broad coverage of conversation topics and to provide moresensitive signals on group-based stereotyping and hate speech. Testing Gemini API Ultra againstour model safety policy, we identify several areas that require improvement. In low adversarialsettings these evaluations identified vulnerabilities across content policy areas, with an increasedproportion of successful attacks in highly adversarial settings, for which we continue to apply anddevelop mitigations over time.\nThese red teaming approaches complement each other in testing capabilities of Gemini models,as well as obtaining coverage of possible queries ranging from casual everyday questions to expertadversarial usage in key areas.\n6A sociotechnical approach is anchored in the observation that AI systems are sociotechnical systems: both humans andtechnological artifacts are necessary in order to make the technology work as intended (Selbst et al., 2019).\n37"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n7.4.3.2Gemini Advanced\nGemini Advanced, which gives access to 1.0 Ultra, has undergone multiple rounds of red-teaming,including safety and persona evaluations. Principles Pioneers, FTE SMEs in multiple domains,calibrated and trained to conduct testing were recruited to test the product; these were conductedby 164 Google testers from 65 office locations in 24 countries who submitted more than 1,400queries/conversations. We also undertook scaled safety evaluations with 100k+ ratings in aggregateacross all policies, neutral-point-of-view evaluations to monitor sensitive topics neutrality and parity,and multiple iterations of Persona evaluations to validate tone.\nWe also enlisted Googlers in a “dogfooding” program, many of which were SMEs in variousdomains, to test across policies and functionality. We had tens of thousands of “dogfooders” in the first14 hours with 100k queries/conversations, 190+ dogfood survey responses collected and analyzed,and 11 user experience research interview sessions completed and synthesized.\nThe results from our red teaming and safety evaluations are used to further strengthen our evalsand improve model performance in an iterative manner.\n7.4.4. External Evaluations\n7.4.4.1Gemini Ultra External Evaluations\nIn 2023, we began working with a small set of independent external groups outside of Google tohelp identify areas for improvement in our model safety work by undertaking structured evaluations,qualitative probing, and unstructured red teaming. External groups were selected based on theirexpertise across a range of domain areas, including those outlined within the White House Commit-ments, the U.S. Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, and theBletchley Declaration:\n• Autonomous replication• Chemical, Biological, Radiological and Nuclear (CBRN) risks• Cyber-capabilities and cyber security• Societal risks, including:\n– Representational and distributional harms– Neutrality and Factuality– Robustness and information hazards.\nGuidance was provided to each external group in relation to the scope of the testing, however,each group independently designed their testing methodology and prompt sets, and wrote theirreports independently of Google. Internal Google experts were on-hand to provide input, whereneeded, based on their experience of testing Gemini models internally.\nExternal groups were given black-box testing access to a December 2023 Gemini API Ultramodel checkpoint over a number of weeks. Access enabled groups to undertake structured, batchedevaluations via the Cloud Vertex AI API or interact with the model via a chat interface, depending onthe type of testing being undertaken. These groups weren’t given access to the pre-trained model,model weights, or queryable or direct external access to our pre-training data.\nThe models tested by external groups were production-ready fine-tuned versions, which hadsafety fine tuning and safety filters applied by default, and the ability to configure some samplingparameters, such as temperature, token limit, Top-k, and Top-p. Groups that did testing via the\n38"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nprogrammatic interface were able to turn down/off some safety filters, however, we wanted themajority of testing by external groups to be undertaken with safety filters in-place because we wantedthe model to be reflective of an end-user’s interaction and were keen to test more than just model-levelsafety.\n7.4.5. Gemini Advanced\nWe undertook three types of external testing on Gemini Advanced:\n• Priority User Program: This program collected feedback from 120 power users, key influencers,and thought-leaders. This program enables the collection of real-time feedback across safetyand other domain areas through the user interface, and where possible, in-depth interviews.Focus areas included safety and persona, functionality, coding and instruction capabilities, andfactuality.• Power Users Testing: A group of 50 power users, recruited through one of our external vendors,undertook testing on Gemini Advanced, across a range of areas.• Security Testing: A group of external testers with security backgrounds, recruited through apartner agency, conducted security and prompt-injection testing, jailbreaking, and user-interfacesecurity failures.\n7.5. Deployment\nFollowing the completion of responsibility and safety reviews, internal model cards (Mitchell et al.,2019) for each approved version of the Gemini model are created for structured and consistent internaldocumentation of critical performance and responsibility metrics as well as to inform appropriateexternal communication of these metrics over time.\nWe release external model and system cards on an ongoing basis within updates of our technicalreports and in documentation for enterprise customers. See Appendix 10.1 for the Gemini Ultramodel card.\nAdditionally, online content covering terms of use, model distribution and access, and operationalaspects such as change control, logging, monitoring and feedback can be found on relevant productwebsites, such as Gemini and Cloud Vertex AI. Some of the key aspects are linked to or describedbelow:\n• Generative AI Prohibited Use Policy• Google Terms of service• Generative AI Terms of service• Google Cloud Platform Terms of service• Gemini Privacy Notice• Google Cloud Privacy Notice"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"8. Discussion and Conclusion\nWe have presented Gemini, a new family of models that advance multimodal model capabilities intext, code, image, audio, and video. Our most capable pre-trained model Gemini Ultra, alongsidethe post-trained Gemini Apps and Gemini API variants, make significant advances across the board.In the natural language domain, the performance gains from careful developments in data andmodel training at scale continue to deliver quality improvements, setting new state of the art in\n39"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nseveral benchmarks. In particular, Gemini Ultra surpasses human-expert performance on the exambenchmark MMLU, scoring 90.0%, which has been a defacto measure of progress for LLMs ever sinceit was first released in 2020. In the multimodal domain, Gemini Ultra sets new state of the art on mostof the image understanding, video understanding, and audio understanding benchmarks withouttask-specific modifications or tuning.In particular, Gemini Ultra’s multimodal reasoning capabilitiesare evident from its state-of-the-art performance on the recent MMMU benchmark (Yue et al., 2023),that comprises questions about images requiring college-level subject knowledge and deliberatereasoning.\nBeyond the state-of-art results on benchmarks, what we are most excited about is the new usecases enabled by Gemini models. The new capabilities of Gemini models to parse complex images,such as charts or infographics, reason over interleaved sequences of images, audio, and text, andgenerate interleaved text and images as responses open a wide variety of new applications. As shownin figures throughout the report and appendix, Gemini models can enable new approaches in areaslike education, everyday problem solving, multilingual communication, information summarization,extraction, and creativity. We expect that the users of these models will find all kinds of beneficialnew uses that we have only scratched the surface of in our own investigations.\nDespite their impressive capabilities, we should note that there are limitations to the use of LLMs.There is a continued need for ongoing research and development on “hallucinations” generated byLLMs to ensure that model outputs are more reliable and verifiable. LLMs also struggle with tasksrequiring high-level reasoning abilities like causal understanding, logical deduction, and counterfactualreasoning even though they achieve impressive performance on exam benchmarks. This underscoresthe need for more challenging and robust evaluations to measure their true understanding as thecurrent state-of-the-art LLMs saturate many benchmarks.\nThe Gemini family is a further step towards our mission to solve intelligence, advance scienceand benefit humanity, and we are enthusiastic to see how these models are used by our colleaguesat Google and beyond. We build on many innovations in machine learning, data, infrastructure,and responsible development – areas that we have been pursuing at Google for over a decade. Themodels we present in this report provide a strong foundation towards our broader future goal todevelop a large-scale, modularized system that will have broad generalization capabilities acrossmany modalities.\n40"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, KarelLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, SerkanCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, SebastianBorgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, RicardoBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual languagemodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736,2022.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent ElShafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick,Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Her-nandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, SiddharthaBrahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin,Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag,Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi,Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah,Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan,Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, HanzhaoLin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, VedantMisra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter,Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, AmbroseSlone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan,Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu,Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng,Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report, 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large languagemodels. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion,Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, TristanHume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei,Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training ahelpful and harmless assistant with reinforcement learning from human feedback. April 2022a.URL https://arxiv.org/abs/2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, KamileLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, NovaDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer ElShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\n41"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nSam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback.arXiv preprint arXiv:2212.08073, 2022b.\nPaul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, MichaelIsard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi,Laurent El Shafey, Chandramohan A. Thekkath, and Yonghui Wu.Pathways: Asynchronousdistributed dataflow for ML. Proceedings of Machine Learning and Systems, 4:430–449, 2022.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. StereotypingNorwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings of the59th Annual Meeting of the Association for Computational Linguistics and the 11th International JointConference on Natural Language Processing (Volume 1: Long Papers), pages 1004–1015, Online,August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.81. URLhttps://aclanthology.org/2021.acl-long.81.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX:composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, JeffreyWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and DarioAmodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, JaredKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, GretchenKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray,Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, ElizabethBarnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, MilesBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXivpreprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\nXi Chen, Xiao Wang, Soravit Changpinyo, A J Piergiovanni, Piotr Padlewski, Daniel Salz, SebastianGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver,Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, JamesBradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme,Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. URL https://arxiv.org/abs/2209.06794.\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-los Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani,\n42"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nDaniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang,Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, FilipPavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, KentonLee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong,Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, andRadu Soricut. PaLI-X: On Scaling up a Multilingual Vision and Language Model. arXiv preprintarXiv:2305.18565, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, KensenShi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer,Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, JacobAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, DennyZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, RyanSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara-narayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov,Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, MicheleCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.PaLM: Scaling Language Modeling with Pathways. Journal of Machine Learning Research, 24(240):1–113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and KristinaToutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings ofthe 2019 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924–2936, 2019. URLhttps://aclanthology.org/N19-1300.\nJon Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, andJennimaria Palomaki. TydiQA: A benchmark for information-seeking question answering in typo-logically diverse languages. Transactions of the Association for Computational Linguistics, 2020. URLhttps://storage.googleapis.com/tydiqa/tydiqa.pdf.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, ChristopherHesse, and John Schulman.Training verifiers to solve math word problems.arXiv preprintarXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.\nAlexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa,Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representationsof speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798–805. IEEE, 2023.\nJeffDean.IntroducingPathways:Anext-generationAIarchi-tecture,2021.URLhttps://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/.\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato,Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances inneural information processing systems, 25, 2012.\nHarish Dattatraya Dixit, Sneha Pendharkar, Matt Beadon, Chris Mason, Tejasvi Chakravarthy, BharathMuthiah, and Sriram Sankar. Silent data corruptions at scale. arXiv preprint arXiv:2102.11245,2021.\n43"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. InICLR, 2020.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. InProceedings of the 2019 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368–2378,2019. URL https://aclanthology.org/N19-1246.\nChristian Federmann, Tom Kocmi, and Ying Xin.NTREX-128 – news test references for MTevaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up MultilingualEvaluation, pages 21–24, Online, nov 2022. Association for Computational Linguistics.URLhttps://aclanthology.org/2022.sumeval-1.4.\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxici-typrompts: Evaluating neural toxic degeneration in language models, 2020.\nAmelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, MaribethRauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, JonathanUesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig,Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando,Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis,Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogueagents via targeted human judgements, 2022. URL https://arxiv.org/abs/2209.14375.\nGoogle.Google’s AI Principles.2023.URL https://ai.google/responsibility/principles/.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQAmatter: Elevating the role of image understanding in visual question answering. In Proceedings ofthe IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summarizationfor 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,pages 4693–4703, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.413. URL https://aclanthology.org/2021.findings-acl.413.\nQianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen,Xintao Wang, Yuncheng Huang, et al. Can large language models understand real-world complexinstructions? arXiv preprint arXiv:2309.09150, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and JacobSteinhardt. Measuring massive multitask language understanding. Proceedings of the InternationalConference on Learning Representations (ICLR), 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. arXivpreprint arXiv:2103.03874, 2021b. URL https://arxiv.org/abs/2103.03874.\n44"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nPeter H Hochschild, Paul Turner, Jeffrey C Mogul, Rama Govindaraju, Parthasarathy Ranganathan,David E Culler, and Amin Vahdat. Cores that don’t count. In Proceedings of the Workshop on HotTopics in Operating Systems, pages 9–16, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-ford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, EricNoland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\nShengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Won’t getfooled again: Answering questions with false premises. arXiv preprint arXiv:2307.02394, 2023.\nEunJeong Hwang and Vered Shwartz. Memecap: A dataset for captioning and interpreting memes,2023.\nNorman P. Jouppi, Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, CliffYoung, and David A. Patterson. A domain-specific supercomputer for training deep neural networks.Commun. ACM, 63(7):67–78, 2020. doi: 10.1145/3360307. URL https://doi.org/10.1145/3360307.\nNorman P Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil,Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, andDavid A Patterson. Tpu v4: An optically reconfigurable supercomputer for machine learning withhardware support for embeddings. In Proceedings of the 50th Annual International Symposium onComputer Architecture, pages 1–14, 2023.\nAshwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, and Peter Clark. HowMuch Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challengefor AI, 2021.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, DragomirRadev, Noah A Smith, Yejin Choi, and Kentaro Inui. Realtime qa: What’s the answer right now?arXiv preprint arXiv:2207.13332, 2022a.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, DragomirRadev, Noah A. Smith, Yejin Choi, and Kentaro Inui. RealTime QA: What’s the answer right now?,2022b. URL https://arxiv.org/abs/2207.13332.\nK Kavukcuoglu, P Kohli, L Ibrahim, D Bloxwich, and S Brown. How our principles helped defineAlphaFold’s release. Google DeepMind, 2022.\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.A diagram is worth a dozen images. In ECCV, 2016.\nMegan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan,Luke Harold Miles, Tao R Lin, Hjalmar Wijk, Joel Burget, et al. Evaluating language-model agentson realistic autonomous tasks. arXiv preprint arXiv:2312.11671, 2023.\nTomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions ofthe Association for Computational Linguistics, 6:317–328, 2018. doi: 10.1162/tacl_a_00023. URLhttps://aclanthology.org/Q18-1023.\n45"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nTom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel,Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, PhilippKoehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák,Martin Popel, and Maja Popović. Findings of the 2022 conference on machine translation (WMT22).In Proceedings of the Seventh Conference on Machine Translation (WMT), December 2022. URLhttps://aclanthology.org/2022.wmt-1.1.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Largelanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subwordtokenizer and detokenizer for neural text processing. In Eduardo Blanco and Wei Lu, editors,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pages 66–71.Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-2012. URL https://doi.org/10.18653/v1/d18-2012.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones,Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.Natural questions: A benchmark for question answering research. Transactions of the Associationfor Computational Linguistics, 7:452–466, 2019a.doi: 10.1162/tacl_a_00276.URL https://aclanthology.org/Q19-1026.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmarkfor question answering research. Transactions of the Association for Computational Linguistics, 7:453–466, 2019b.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmarkdataset for cross-lingual abstractive summarization. In Findings of the Association for ComputationalLinguistics: EMNLP 2020, pages 4034–4048, Online, November 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.findings-emnlp.360. URL https://www.aclweb.org/anthology/2020.findings-emnlp.360.\nLeblond et al. AlphaCode 2 Technical Report. 2023. URL https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, TomEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generationwith alphacode. Science, 378(6624):1092–1097, 2022.\nBin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visualrepresentation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.\nFangyu Liu, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee,Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. DePlot: One-shot visual languagereasoning by plot-to-table translation. In Findings of the Association for Computational Linguistics:ACL 2023, pages 10381–10399, Toronto, Canada, July 2023. Association for ComputationalLinguistics. doi: 10.18653/v1/2023.findings-acl.660. URL https://aclanthology.org/2023.findings-acl.660.\n46"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nPan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning.In The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguisticsand the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021),2021.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning offoundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023.\nAhmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark forquestion answering about charts with visual and logical reasoning. In Findings of ACL, 2022.\nMinesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on documentimages. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages2200–2209, 2021.\nMinesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar.Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,pages 1697–1706, 2022.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factualityin abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, MiaGlaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. Teachinglanguage models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conductelectricity? a new dataset for open book question answering. In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing, pages 2381–2391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URLhttps://aclanthology.org/D18-1260.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalizationvia natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773, 2021.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary!topic-aware convolutional neural networks for extreme summarization. In Proceedings of the2018 Conference on Empirical Methods in Natural Language Processing, pages 1797–1807, Brussels,Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206.\nOktatási Hivatal.Matematika írásbéli vizsga.Középszintű Írásbéli Vizsga, May 2023.URLhttps://dload-oktatas.educatio.hu/erettsegi/feladatok_2023tavasz_kozep/k_matang_23maj_fl.pdf. Angol Nyelven.\nOpenAI. GPT-4 Technical Report. 2023a.\nOpenAI. GPT-4V(ision) System Card, 2023b.\nOpenAI. Whisper, 2023. URL https://github.com/openai/whisper.\n47"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton,Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and RyanLowe. Training language models to follow instructions with human feedback. In Alice H. Oh, AlekhAgarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information ProcessingSystems, 2022. URL https://openreview.net/forum?id=TG8KACxEON.\nDenis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, SandroPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Wordprediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\nAlicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson,Phu Mon Htut, and Samuel R. Bowman. BBQ: A hand-built bias benchmark for question answering.CoRR, abs/2110.08193, 2021. URL https://arxiv.org/abs/2110.08193.\nViorica Pătrăucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, DylanBanarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, TatianaMatejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, JunlinZhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, andJoăo Carreira. Perception test: A diagnostic benchmark for multimodal video models. arXiv preprintarXiv:2305.13786, 2023.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden,Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language modelswith external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.\nLeon Poutievski, Omid Mashayekhi, Joon Ong, Arjun Singh, Mukarram Tariq, Rui Wang, Jianan Zhang,Virginia Beauregard, Patrick Conner, Steve Gribble, et al. Jupiter evolving: transforming google’sdatacenter network via optical circuit switches and software-defined networking. In Proceedings ofthe ACM SIGCOMM 2022 Conference, pages 66–85, 2022.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: Alarge-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.URLhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.Robust speech recognition via large-scale weak supervision. In International Conference on MachineLearning, pages 28492–28518. PMLR, 2023.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song,John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, SaffronHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, KarenSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, MantasPajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun\n48"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nTerzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones,James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S.Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scalinglanguage models: Methods, analysis & insights from training Gopher. CoRR, abs/2112.11446,2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on MachineLearning, pages 8821–8831. PMLR, 2021.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, SlavPetrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural languagegeneration models. Computational Linguistics, pages 1–64, 2023.\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, GabrielBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, JakeBruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, MahyarBordbar, and Nando de Freitas. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.\nParker Riley, Timothy Dozat, Jan A Botha, Xavier Garcia, Dan Garrette, Jason Riesa, Orhan Firat, andNoah Constant. Frmt: A benchmark for few-shot region-aware machine translation. Transactions ofthe Association for Computational Linguistics, 2023.\nHannah Ritchie, Veronika Samborska, and Max Roser. Plastic pollution. Our World in Data, 2023.https://ourworldindata.org/plastic-pollution.\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into theparameters of a language model? In Proceedings of the 2020 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages 5418–5426, Online, November 2020. Associ-ation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020.emnlp-main.437.\nWilliam A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, andCody Coleman. The dollar street dataset: Images representing the geographic and socioeconomicdiversity of the world. In Thirty-sixth Conference on Neural Information Processing Systems Datasetsand Benchmarks Track, 2022.\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias incoreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers),pages 8–14, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:10.18653/v1/N18-2002. URL https://aclanthology.org/N18-2002.\nCandice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Pantofaru. A steptoward more inclusive people annotations for fairness. In Proceedings of the 2021 AAAI/ACMConference on AI, Ethics, and Society, pages 916–925, 2021.\nAndrew D. Selbst, Danah Boyd, and Sorelle A. Friedler. Fairness and abstraction in sociotechnicalsystems. In FFAT* ’19: Proceedings of the Conference on Fairness, Accountability, and Transparency,pages 59–68, January 2019.\n49"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nThibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation.In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages7881–7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https://aclanthology.org/2020.acl-main.704.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, MorGeva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long languagesequences.In Proceedings of the 2022 Conference on Empirical Methods in Natural LanguageProcessing, pages 12007–12021, Abu Dhabi, United Arab Emirates, December 2022. Association forComputational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.\nNoam Shazeer.Fast transformer decoding: One write-head is all you need.arXiv preprintarXiv:1911.02150, 2019a.\nNoam Shazeer.Fast transformer decoding: One write-head is all you need.arXiv preprintarXiv:1911.02150, 2019b.\nRenee Shelby, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar Rostamzadeh, Paul Nicholas,N’Mah Yilla, Jess Gallegos, Andrew Smart, Emilio Garcia, and Gurleen Virk.Identifying so-ciotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction, 2023. URLhttps://arxiv.org/abs/2210.05791.\nToby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung,Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth,Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, PaulChristiano, and Allan Dafoe. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324,2023.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung WonChung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. ICLR, 2023.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, andMarcus Rohrbach. Towards VQA models that can read. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 8317–8326, 2019.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R. Brown, et al. Beyond the imitation game: Quantifying and extrapolating the capabilitiesof language models. arXiv preprint arXiv:2206.04615, 2022. URL https://arxiv.org/abs/2206.04615.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.Advances in neural information processing systems, 27, 2014.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks andwhether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proof Writer: Generating implications, proofs,and abductive statements over natural language.In Findings, 2020.URL https://api.semanticscholar.org/CorpusID:229371222.\n50"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield,Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang,Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, PrangthipHansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit,Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, CynthiaGao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, ChristopheRopers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scalinghuman-centered machine translation. 2022.\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A massivelymultilingual multimodal evaluation dataset. In EMNLP, 2022.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications.arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.\nKocmi Tom, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, ChristianFedermann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. Findingsof the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet. InWMT23-Eighth Conference on Machine Translation, pages 198–216, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothéeLacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, ArmandJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation languagemodels. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Can-ton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu,Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Ko-renev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, YinghaiLu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric MichaelSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian XiangKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, SharanNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukaszKaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wal-lach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information ProcessingSystems, volume 30. Curran Associates, Inc., 2017a. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017b. URLhttp://arxiv.org/abs/1706.03762.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image descrip-tion evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 4566–4575, 2015.\n51"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nPetar Veličković, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino, MishaDashevskiy, Raia Hadsell, and Charles Blundell. The clrs algorithmic reasoning benchmark. arXivpreprint arXiv:2205.15659, 2022.\nManoj Vishwanathan, Ronak Shah, Kyung Ki Kim, and Minsu Choi. Silent data corruption (sdc)vulnerability of gpu on various gpgpu workloads. In 2015 International SoC Design Conference(ISOCC), pages 11–12, 2015. doi: 10.1109/ISOCC.2015.7401681.\nChanghan Wang, Anne Wu, and Juan Pino. Covost 2 and massively multilingual speech-to-texttranslation. arXiv preprint arXiv:2007.10310, 2020.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, MaryWilliamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speechcorpus for representation learning, semi-supervised learning and interpretation. arXiv preprintarXiv:2101.00390, 2021.\nXin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. VATEX: Alarge-scale, high-quality multilingual dataset for video-and-language research. In ICCV, 2019.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistencyimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. Proceedings of theInternational Conference on Learning Representations (ICLR), 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS,2022b. URL https://arxiv.org/abs/2201.11903.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, MyraCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, TomStepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S.Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm fromlanguage models. CoRR, abs/2112.04359, 2021. URL https://arxiv.org/abs/2112.04359.\nDavid Wetherall, Abdul Kabbani, Van Jacobson, Jim Winget, Yuchung Cheng, Brad Morrey,Uma Parthavi Moravapalle, Phillipa Gill, Steven Knight, and Amin Vahdat. Improving networkavailability with protective reroute. In SIGCOMM 2023, 2023. URL https://dl.acm.org/doi/10.1145/3603269.3604867.\nJunbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-answeringto explaining temporal actions. In CVPR, 2021.\nXLA. XLA: Optimizing compiler for TensorFlow. https://www.tensorflow.org/xla, 2019.[Online; accessed December-2023].\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and DaxinJiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprintarXiv:2304.12244, 2023.\nYuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, MaximKrikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable paral-lelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.\n52"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nChi yao Hong, Subhasree Mandal, Mohammad A. Alfares, Min Zhu, Rich Alimi, Kondapa NaiduBollineni, Chandan Bhagat, Sourabh Jain, Jay Kaimal, Jeffrey Liang, Kirill Mendelev, Steve Padgett,Faro Thomas Rabe, Saikat Ray, Malveeka Tewari, Matt Tierney, Monika Zahn, Jon Zolla, JoonOng, and Amin Vahdat. B4 and after: Managing hierarchy, partitioning, and asymmetry foravailability and scale in google’s software-defined wan. In SIGCOMM’18, 2018. URL https://conferences.sigcomm.org/sigcomm/2018/program_tuesday.html.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:Contrastive captioners are image-text foundation models, 2022a.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, XinLi, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-richtext-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022b.\nShoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model forvideo localization and question answering. arXiv preprint arXiv:2305.06988, 2023.\nZhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. ActivityNet-QA:A dataset for understanding complex web videos via question answering. In AAAI, 2019.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, DongfuJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expertagi, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machinereally finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\nYu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen,Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar,Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, BhuvanaRamabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, FrançoiseBeaufays, and Yonghui Wu. Google usm: Scaling automatic speech recognition beyond 100languages. arXiv preprint arXiv:2303.01037, 2023.\nDora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases inimage captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 14830–14840, 2021.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias incoreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint promptingimproves reasoning in large language models, 2023.\nLuowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from webinstructional videos. In AAAI Conference on Artificial Intelligence, pages 7590–7598, 2018.\n53"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n54\n"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n\n9. Contributions and Acknowledgments\nGemini LeadsRohan Anil, Co-Lead, TextSebastian Borgeaud, Co-Lead, TextJean-Baptiste Alayrac, Co-Lead, MM VisionJiahui Yu, Co-Lead, MM VisionRadu Soricut, Co-Lead, MM VisionJohan Schalkwyk, Lead, MM AudioAndrew M. Dai, Co-Lead, DataAnja Hauth, Co-Lead, DataKatie Millican, Co-Lead, DataDavid Silver, Co-Lead, Fine-TuningMelvin Johnson, Lead, Instruction TuningIoannis Antonoglou, Co-Lead, RL TechniquesJulian Schrittwieser, Co-Lead, RL TechniquesAmelia Glaese, Lead, Human DataJilin Chen, Lead, SafetyEmily Pitler, Co-Lead, Tool UseTimothy Lillicrap, Co-Lead, Tool UseAngeliki Lazaridou, Co-Lead, EvalOrhan Firat, Co-Lead, EvalJames Molloy, Co-Lead, InfraMichael Isard, Co-Lead, InfraPaul R. Barham, Co-Lead, InfraTom Hennigan, Co-Lead, InfraBenjamin Lee, Co-Lead, Codebase & ParallelismFabio Viola, Co-Lead, Codebase & ParallelismMalcolm Reynolds, Co-Lead, Codebase & ParallelismYuanzhong Xu, Co-Lead, Codebase & ParallelismRyan Doherty, Lead, EcosystemEli Collins, Lead, ProductClemens Meyer, Co-Lead, OperationsEliza Rutherford, Co-Lead, OperationsErica Moreira, Co-Lead, OperationsKareem Ayoub, Co-Lead, OperationsMegha Goel, Co-Lead, Operations\nGemini App LeadsJack Krawczyk, Lead, Gemini App ProductCosmo Du, Co-Lead, Gemini App ResearchEd Chi, Co-Lead, Gemini App ResearchHeng-Tze Cheng, Co-Lead, Gemini App ResearchEric Ni, Lead, Gemini App Research Technical ProgramManagementPurvi Shah, Lead, Gemini App Technical ProgramManagementPatrick Kane, Co-Lead, Gemini App Core Modeling, Eval,Data, ProductBetty Chan, Co-Lead, Gemini App Core Modeling,Technical Program Management\nManaal Faruqui,Co-Lead,Gemini App CoreModeling, Factuality, Instruction FollowingAliaksei Severyn,Co-Lead,Gemini App CoreModeling, ConversationalityHanzhao Lin, Co-Lead, Gemini App Fine-TuningYaGuang Li, Co-Lead, Gemini App Fine-TuningYong Cheng, Co-Lead, Gemini App Fine-TuningAbe Ittycheriah, Co-Lead, Gemini for Gemini AppMahdis Mahdieh, Co-Lead, Gemini for Gemini AppMia Chen, Co-Lead, Gemini for Gemini AppPei Sun, Co-Lead, Gemini for Gemini AppDustin Tran, Co-Lead, Gemini App EvalSumit Bagri, Co-Lead, Gemini App Eval, TechnicalProgram ManagementBalaji Lakshminarayanan, Co-Lead, Gemini AppAutoEvalJeremiah Liu, Co-Lead, Gemini App AutoEvalAndras Orban, Co-Lead, Gemini App Factuality,Multimodality, SafetyFabian Güra, Co-Lead, Gemini App FactualityHao Zhou, Co-Lead, Gemini App FactualityXinying Song, Co-Lead, Gemini App FactualityAurelien Boffy, Co-Lead, Gemini App SafetyHarish Ganapathy, Co-Lead, Gemini SafetySteven Zheng, Lead, Gemini App MultilingualityResearchHyunJeong Choe, Lead, Gemini App MultilingualityÁgoston Weisz, Co-Lead, Gemini App MultimodalityTao Zhu, Co-Lead, Gemini App MultimodalityYifeng Lu, Co-Lead, Gemini App MultimodalitySiddharth Gopal, Co-Lead, Gemini App Coding &Tool UseJarrod Kahn, Co-Lead, Gemini App Tool UseResearchMaciej Kula, Co-Lead, Gemini App Tool Use ResearchJeff Pitman, Co-Lead, Gemini App Tool UseRushin Shah, Co-Lead, Gemini App Tool UseEmanuel Taropa, Co-Lead, Gemini App ServingMajd Al Merey, Co-Lead, Gemini App ServingMartin Baeuml, Co-Lead, Gemini App ServingZhifeng Chen, Co-Lead, Gemini App ServingLaurent El Shafey, Co-Lead, Gemini App Fine-TuningInfraYujing Zhang, Co-Lead, Gemini App Fine-TuningInfraOlcan Sercinoglu, Lead, Gemini App Product\n55"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsGeorge TuckerEnrique PiquerasMaxim KrikunIain BarrNikolay SavinovIvo DanihelkaBecca RoelofsAnaïs WhiteAnders AndreassenTamara von GlehnLakshman YagatiMehran KazemiLucas GonzalezMisha KhalmanJakub SygnowskiAlexandre FrechetteCharlotte SmithLaura CulpLev ProleevYi LuanXi ChenJames LottesNathan SchucherFederico LebronAlban RrustemiNatalie ClayPhil CroneTomas KociskyJeffrey ZhaoBartek PerzDian YuHeidi HowardAdam BloniarzJack W. RaeHan LuLaurent SifreMarcello MaggioniFred AlcoberDan GarretteMegan BarnesShantanu ThakoorJacob AustinGabriel Barth-MaronWilliam WongRishabh JoshiRahma ChaabouniDeeni FatihaArun Ahuja\nCore ContributorsGaurav Singh TomarEvan SenterMartin ChadwickIlya KornakovNithya AttaluriIñaki IturrateRuibo LiuYunxuan LiSarah CoganJeremy ChenChao JiaChenjie GuQiao ZhangJordan GrimstadAle Jakse HartmanXavier GarciaThanumalayan Sankaranarayana PillaiJacob DevlinMichael LaskinDiego de Las CasasDasha ValterConnie TaoLorenzo BlancoAdrià Puigdomènech BadiaDavid ReitterMianna ChenJenny BrennanClara RiveraSergey BrinShariq IqbalGabriela SuritaJane LabanowskiAbhi RaoStephanie WinklerEmilio ParisottoYiming GuKate OlszewskaRavi AddankiAntoine MiechAnnie LouisDenis TeplyashinGeoff BrownElliot CattJan BalaguerJackie XiangPidong WangZoe AshwoodAnton Briukhov\n56"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsAlbert WebsonSanjay GanapathySmit SanghaviAjay KannanMing-Wei ChangAxel StjerngrenJosip DjolongaYuting SunAnkur BapnaMatthew AitchisonPedram PejmanHenryk MichalewskiTianhe YuCindy WangJuliette LoveJunwhan AhnDawn BloxwichKehang HanPeter HumphreysThibault SellamJames BradburyVarun GodboleSina SamangooeiBogdan DamocAlex KaskasoliSébastien M. R. ArnoldVijay VasudevanShubham AgrawalJason RiesaDmitry LepikhinRichard TanburnSrivatsan SrinivasanHyeontaek LimSarah HodkinsonPranav ShyamJohan FerretSteven HandAnkush GargTom Le PaineJian LiYujia LiMinh GiangAlexander NeitzZaheer AbbasSarah YorkMachel ReidElizabeth ColeAakanksha Chowdhery\nCore ContributorsDipanjan DasDominika RogozińskaVitaly NikolaevPablo SprechmannZachary NadoLukas ZilkaFlavien ProstLuheng HeMarianne MonteiroGaurav MishraChris WeltyJosh NewlanDawei JiaMiltiadis AllamanisClara Huiyi HuRaoul de LiedekerkeJustin GilmerCarl SaroufimShruti RijhwaniShaobo HouDisha ShrivastavaAnirudh BaddepudiAlex GoldinAdnan OzturelAlbin CassirerYunhan XuDaniel SohnDevendra SachanReinald Kim AmplayoCraig SwansonDessie PetrovaShashi NarayanArthur GuezSiddhartha BrahmaJessica LandonMiteyan PatelRuizhe ZhaoKevin VillelaLuyu WangWenhao JiaMatthew RahtzMai GiménezLegg YeungJames KeelingPetko GeorgievDiana MincuBoxi WuSalem Haykal\n57"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsRachel SaputroKiran VodrahalliJames QinZeynep CankaraAbhanshu SharmaNick FernandoWill HawkinsBehnam NeyshaburSolomon KimAdrian HutterPriyanka AgrawalAlex Castro-RosGeorge van den DriesscheTao WangFan YangShuo-yiin ChangPaul KomarekRoss McIlroyMario LučićGuodong ZhangWael FarhanMichael SharmanPaul NatsevPaul MichelYamini BansalSiyuan QiaoKris CaoSiamak ShakeriChristina ButterfieldJustin ChungPaul Kishan RubensteinShivani AgrawalArthur MenschKedar SoparkarKarel LencTimothy ChungAedan PopeLoren MaggioreJackie KayPriya JhakraShibo WangJoshua MaynezMary PhuongTaylor TobinAndrea TacchettiMaja TrebaczKevin RobinsonYash Katariya\nCore ContributorsSebastian RiedelPaige BaileyKefan XiaoNimesh GhelaniLora AroyoAmbrose SloneNeil HoulsbyXuehan XiongZhen YangElena GribovskayaJonas AdlerMateo WirthLisa LeeMusic LiThais KagoharaJay PavagadhiSophie BridgersAnna BortsovaSanjay GhemawatZafarali AhmedTianqi LiuRichard PowellVijay BolinaMariko IinumaPolina ZablotskaiaJames BesleyDa-Woon ChungTimothy DozatRamona ComanescuXiance SiJeremy GreerGuolong SuMartin PolacekRaphaël Lopez KaufmanSimon TokumineHexiang HuElena BuchatskayaYingjie MiaoMohamed ElhawatyAditya SiddhantNenad TomasevJinwei XingChristina GreerHelen MillerShereen AshrafAurko RoyZizhao ZhangAda Ma\n58"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsAngelos FilosMilos BestaRory BlevinsTed KlimenkoChih-Kuan YehSoravit ChangpinyoJiaqi MuOscar ChangMantas PajarskasCarrie MuirVered CohenCharline Le LanKrishna HaridasanAmit MaratheSteven HansenSholto DouglasRajkumar SamuelMingqiu WangSophia AustinChang LanJiepu JiangJustin ChiuJaime Alonso LorenzoLars Lowe SjösundSébastien CeveyZach GleicherThi AvrahamiAnudhyan BoralHansa SrinivasanVittorio SeloRhys MayKonstantinos AisoposLéonard HussenotLivio Baldini SoaresKate BaumliMichael B. ChangAdrià RecasensBen CaineAlexander PritzelFilip PaveticFabio PardoAnita GergelyJustin FryeVinay RamaseshDan HorganKartikeya BadolaNora KassnerSubhrajit Roy\nCore ContributorsEthan DyerVíctor CamposAlex TomalaYunhao TangDalia El BadawyElspeth WhiteBasil MustafaOran LangAbhishek JindalSharad VikramZhitao GongSergi CaellesRoss HemsleyGregory ThorntonFangxiaoyu FengWojciech StokowiecCe ZhengPhoebe ThackerÇağlar ÜnlüZhishuai ZhangMohammad SalehJames SvenssonMax BileschiPiyush PatilAnkesh AnandRoman RingKaterina TsihlasArpi VezerMarco SelviToby ShevlaneMikel RodriguezTom KwiatkowskiSamira DarukiKeran RongAllan DafoeNicholas FitzGeraldKeren Gu-LembergMina KhanLisa Anne HendricksMarie PellatVladimir FeinbergJames Cobon-KerrTara SainathMaribeth RauhSayed Hadi HashemiRichard IvesYana HassonEric Noland\n59"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsYuan CaoNathan ByrdLe HouQingze WangThibault SottiauxMichela PaganiniJean-Baptiste LespiauAlexandre MoufarekSamer HassanKaushik ShivakumarJoost van AmersfoortAmol MandhanePratik JoshiAnirudh GoyalMatthew TungAndrew BrockHannah SheahanVedant MisraCheng LiNemanja RakićevićMostafa DehghaniFangyu LiuSid MittalJunhyuk OhSeb NouryEren SezenerFantine HuotMatthew LammNicola De CaoCharlie ChenSidharth MudgalRomina StellaKevin BrooksGautam VasudevanChenxi LiuMainak ChainNivedita MelinkeriAaron CohenVenus WangKristie SeymoreSergey ZubkovRahul GoelSummer YueSai KrishnakumaranBrian AlbertNate HurleyMotoki SanoAnhad Mohananey\nCore ContributorsJonah JoughinEgor FilonovTomasz KępaYomna EldawyJiawern LimRahul RishiShirin BadiezadeganTaylor BosJerry ChangSanil JainSri Gayatri Sundara PadmanabhanSubha PuttaguntaKalpesh KrishnaLeslie BakerNorbert KalbVamsi BedapudiAdam KurzrokShuntong LeiAnthony YuOren LitvinXiang ZhouZhichun WuSam SobellAndrea SicilianoAlan PapirRobby NealeJonas BragagnoloTej ToorTina ChenValentin AnklinFeiran WangRichie FengMilad GholamiKevin LingLijuan LiuJules WalterHamid MoghaddamArun KishoreJakub AdamekTyler MercadoJonathan MallinsonSiddhinita WandekarStephen CagleEran OfekGuillermo GarridoClemens LombriserMaksim MukhaBotu Sun\n60"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsHafeezul Rahman MohammadJosip MatakYadi QianVikas PeswaniPawel JanusQuan YuanLeif SchelinOana DavidAnkur GargYifan HeOleksii DuzhyiAnton ÄlgmyrTimothée LottazQi LiVikas YadavLuyao XuAlex ChinienRakesh ShivannaAleksandr ChuklinJosie LiCarrie SpadineTravis WolfeKareem MohamedSubhabrata DasZihang DaiKyle HeDaniel von DincklageShyam UpadhyayAkanksha MauryaLuyan ChiSebastian KrauseKhalid SalamaPam G RabinovitchPavan Kumar Reddy MAarush SelvanMikhail DektiarevGolnaz GhiasiErdem GuvenHimanshu GuptaBoyi LiuDeepak SharmaIdan Heimlich ShtacherShachi PaulOscar AkerlundFrançois-Xavier AubetTerry HuangChen ZhuEric Zhu\nCore ContributorsElico TeixeiraMatthew FritzeFrancesco BertoliniLiana-Eleonora MarinescuMartin BölleDominik PaulusKhyatti GuptaTejasi LatkarMax ChangJason SandersRoopa WilsonXuewei WuYi-Xuan TanLam Nguyen ThietTulsee DoshiSid LallSwaroop MishraWanming ChenThang LuongSeth BenjaminJasmine (Sun Jae) LeeEwa AndrejczukDominik RabiejVipul RanjanKrzysztof StyrcPengcheng YinJon SimonMalcolm Rose HarriottMudit BansalAlexei RobskyGeoff BaconDavid GreeneDaniil MirylenkaChen ZhouObaid SarvanaAbhimanyu GoyalSamuel AndermattPatrick SieglerBen HornAssaf IsraelFrancesco PongettiChih-Wei “Louis” ChenMarco SelvaticiPedro SilvaKathie WangJackson TolinsKelvin GuuRoey Yogev\n61"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsXiaochen CaiAlessandro AgostiniMaulik ShahHung NguyenNoah Ó DonnaileSébastien PereiraLinda FrisoAdam StamblerAdam KurzrokChenkai KuangYan RomanikhinMark GellerZJ YanKane JangCheng-Chun LeeWojciech FicaEric MalmiQijun TanDan BanicaDaniel BalleRyan PhamYanping HuangDiana AvramHongzhi ShiJasjot SinghChris HideyNiharika AhujaPranab SaxenaDan DooleySrividya Pranavi PotharajuEileen O’NeillAnand GokulchandranRyan FoleyKai ZhaoMike DusenberryYuan LiuPulkit MehtaRagha KotikalapudiChalence Safranek-ShraderAndrew GoodmanJoshua KessingerEran GlobenPrateek KolharChris GorgolewskiAli IbrahimYang SongAli EichenbaumThomas Brovelli\nCore ContributorsSahitya PotluriPreethi LahotiCip BaetuAli GhorbaniCharles ChenAndy CrawfordShalini PalMukund SridharPetru GuritaAsier MujikaIgor PetrovskiPierre-Louis CedozChenmei LiShiyuan ChenNiccolò Dal SantoSiddharth GoyalJitesh PunjabiKarthik KappaganthuChester KwakPallavi LVSarmishta VeluryHimadri ChoudhuryJamie HallPremal ShahRicardo FigueiraMatt ThomasMinjie LuTing ZhouChintu KumarThomas JurdiSharat ChikkerurYenai MaAdams YuSoo KwakVictor ÄhdelSujeevan RajayogamTravis ChomaFei LiuAditya BaruaColin JiJi Ho ParkVincent HellendoornAlex BaileyTaylan BilalHuanjie ZhouMehrdad KhatirCharles SuttonWojciech Rzadkowski\n62"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nCore ContributorsFiona MacintoshRoopali VijKonstantin ShaginPaul MedinaChen LiangJinjing ZhouPararth ShahYingying BiAttila DankovicsShipra BangaSabine LehmannMarissa BredesenZifan LinJohn Eric HoffmannJonathan LaiRaynald ChungKai YangNihal BalaniArthur BražinskasAndrei SozanschiMatthew HayesHéctor Fernández AlcaldePeter MakarovWill ChenAntonio StellaLiselotte SnijdersMichael MandlAnte KärrmanPaweł NowakXinyi WuAlex DyckKrishnan VaidyanathanRaghavender RJessica MalletMitch RudominerEric JohnstonSushil MittalAkhil UdathuJanara ChristensenVishal VermaZach IrvingAndreas Santucci\nContributorsGamaleldin ElsayedElnaz DavoodiMarin GeorgievIan Tenney\nContributorsNan HuaGeoffrey CideronEdouard LeurentMahmoud AlnahlawiIonut GeorgescuNan WeiIvy ZhengDylan ScandinaroHeinrich JiangJasper SnoekMukund SundararajanXuezhi WangZack OntiverosItay KaroJeremy ColeVinu RajashekharLara TumehEyal Ben-DavidRishub JainJonathan UesatoRomina DattaOskar BunyanShimu WuJohn ZhangPiotr StanczykYe ZhangDavid SteinerSubhajit NaskarMichael AzzamMatthew JohnsonAdam PaszkeChung-Cheng ChiuJaume Sanchez EliasAfroz MohiuddinFaizan MuhammadJin MiaoAndrew LeeNino VieillardJane ParkJiageng ZhangJeff StanwayDrew GarmonAbhijit KarmarkarZhe DongJong LeeAviral KumarLuowei ZhouJonathan Evens\n63"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nContributorsWilliam IsaacGeoffrey IrvingEdward LoperMichael FinkIsha ArkatkarNanxin ChenIzhak ShafranIvan PetrychenkoZhe ChenJohnson JiaAnselm LevskayaZhenkai ZhuPeter GrabowskiYu MaoAlberto MagniKaisheng YaoJavier SnaiderNorman CasagrandeEvan PalmerPaul SuganthanAlfonso CastañoIrene GiannoumisWooyeol KimMikołaj RybińskiAshwin SreevatsaJennifer PrendkiDavid SoergelAdrian GoedeckemeyerWilli GierkeMohsen JafariMeenu GabaJeremy WiesnerDiana Gage WrightYawen WeiHarsha VashishtYana KulizhskayaJay HooverMaigo LeLu LiChimezie IwuanyanwuLu LiuKevin RamirezAndrey KhorlinAlbert CuiTian LINMarcus WuRicardo AguilarKeith Pallo\nContributorsAbhishek ChakladarGinger PerngElena Allica AbellanMingyang ZhangIshita DasguptaNate KushmanIvo PenchevAlena RepinaXihui WuTom van der WeidePriya PonnapalliCaroline KaplanJiri SimsaShuangfeng LiOlivier DousseFan YangJeff PiperNathan IeRama PasumarthiNathan LintzAnitha VijayakumarDaniel AndorPedro ValenzuelaMinnie LuiCosmin PaduraruDaiyi PengKatherine LeeShuyuan ZhangSomer GreeneDuc Dung NguyenPaula KurylowiczCassidy HardinLucas DixonLili JanzerKiam ChooZiqiang FengBiao ZhangAchintya SinghalDayou DuDan McKinnonNatasha AntropovaTolga BolukbasiOrgad KellerDavid ReidDaniel FinchelsteinMaria Abi RaadRemi CrockerPeter Hawkins\n64"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nContributorsRobert DadashiColin GaffneyKen FrankoAnna BulanovaRémi LeblondShirley ChungHarry AskhamLuis C. CoboKelvin XuFelix FischerJun XuChristina SorokinChris AlbertiChu-Cheng LinColin EvansAlek DimitrievHannah ForbesDylan BanarseZora TungMark OmernickColton BishopRachel SterneckRohan JainJiawei XiaEhsan AmidFrancesco PiccinnoXingyu WangPraseem BanzalDaniel J. MankowitzAlex PolozovVictoria KrakovnaSasha BrownMohammadHossein BateniDennis DuanVlad FiroiuMeghana ThotakuriTom NatanMatthieu GeistSertan GirginHui LiJiayu YeOfir RovalReiko TojoMichael KwongJames Lee-ThorpChristopher YewDanila SinopalnikovSabela Ramos\nContributorsJohn MellorAbhishek SharmaKathy WuDavid MillerNicolas SonneratDenis VnukovRory GreigJennifer BeattieEmily CavenessLibin BaiJulian EisenschlosAlex KorchemniyTomy TsaiMimi JasarevicWeize KongPhuong DaoZeyu ZhengFrederick LiuFan YangRui ZhuTian Huey TehJason SanmiyaEvgeny GladchenkoNejc TrdinDaniel ToyamaEvan RosenSasan TavakkolLinting XueChen ElkindOliver WoodmanJohn CarpenterGeorge PapamakariosRupert KempSushant KafleTanya GruninaRishika SinhaAlice TalbertDiane WuDenese Owusu-AfriyieCosmo DuChloe ThorntonJordi Pont-TusetPradyumna NarayanaJing LiSaaber FatehiJohn WietingOmar AjmeriBenigno Uria\n65"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nContributorsYeongil KoLaura KnightAmélie HéliouNing NiuShane GuChenxi PangYeqing LiNir LevineAriel StolovichRebeca Santamaria-FernandezSonam GoenkaWenny YustalimRobin StrudelAli ElqurshCharlie DeckHyo LeeZonglin LiKyle LevinRaphael HoffmannDan Holtmann-RiceOlivier BachemSho AroraChristy KohSoheil Hassas YeganehSiim PõderMukarram TariqYanhua SunLucian IonitaMojtaba SeyedhosseiniPouya TaftiZhiyu LiuAnmol GulatiJasmine LiuXinyu YeBart ChrzaszczLily WangNikhil SethiTianrun LiBen BrownShreya SinghWei FanAaron ParisiJoe StantonVinod KoverkathuChristopher A. Choquette-ChooYunjie LiTJ LuAbe Ittycheriah\nContributorsPrakash ShroffMani VaradarajanSanaz BahargamRob WilloughbyDavid GaddyGuillaume DesjardinsMarco CorneroBrona RobenekBhavishya MittalBen AlbrechtAshish ShenoyFedor MoiseevHenrik JacobssonAlireza GhaffarkhahMorgane RivièreAlanna WaltonClément CrepyAlicia ParrishZongwei ZhouClement FarabetCarey RadebaughPraveen SrinivasanClaudia van der SalmAndreas FidjelandSalvatore ScellatoEri Latorre-ChimotoHanna Klimczak-PlucińskaDavid BridsonDario de CesareTom HudsonPiermaria MendolicchioLexi WalkerAlex MorrisMatthew MaugerAlexey GuseynovAlison ReidSeth OdoomLucia LoherVictor CotrutaMadhavi YenugulaDominik GreweAnastasia PetrushkinaTom DuerigAntonio SanchezSteve YadlowskyAmy ShenAmir GlobersonLynette Webb\n66"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nContributorsSahil DuaDong LiSurya BhupatirajuDan HurtHaroon QureshiAnanth AgarwalTomer ShaniMatan EyalAnuj KhareShreyas Rammohan BelleLei WangChetan TekurMihir Sanjay KaleJinliang WeiRuoxin SangBrennan SaetaTyler LiechtyYi SunYao ZhaoStephan LeePandu NayakDoug FritzManish Reddy VuyyuruJohn AslanidesNidhi VyasMartin WickeXiao MaEvgenii EltyshevNina MartinHardie CateJames ManyikaKeyvan AmiriYelin KimXi XiongKai KangFlorian LuisierNilesh TripuraneniDavid MadrasMandy GuoAustin WatersOliver WangJoshua AinslieJason BaldridgeHan ZhangGarima PruthiJakob BauerFeng YangRiham Mansour\nContributorsJason GelmanYang XuGeorge PolovetsJi LiuHonglong CaiWarren ChenXiangHai ShengEmily XueSherjil OzairChristof AngermuellerXiaowei LiAnoop SinhaWeiren WangJulia WiesingerEmmanouil KoukoumidisYuan TianAnand IyerMadhu GurumurthyMark GoldensonParashar ShahMK BlakeHongkun YuAnthony UrbanowiczJennimaria PalomakiChrisantha FernandoKen DurdenHarsh MehtaNikola MomchevElahe RahimtoroghiMaria GeorgakiAmit RaulSebastian RuderMorgan RedshawJinhyuk LeeDenny ZhouKomal JalanDinghua LiBlake HechtmanParker SchuhMilad NasrKieran MilanVladimir MikulikJuliana FrancoTim GreenNam NguyenJoe KelleyAroma MahendruAndrea Hu\n67"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nContributorsJoshua HowlandBen VargasJeffrey HuiKshitij BansalVikram RaoRakesh GhiyaEmma WangKe YeJean Michel SarrMelanie Moranski PrestonMadeleine ElishSteve LiAakash KakuJigar GuptaIce PasupatDa-Cheng JuanMilan SomeswarTejvi M.Xinyun ChenAida AminiAlex FabrikantEric ChuXuanyi DongAmruta MuthalSenaka ButhpitiyaSarthak JauhariNan HuaUrvashi KhandelwalAyal HitronJie RenLarissa RinaldiShahar DrathAvigail DabushNan-Jiang JiangHarshal GodhiaUli SachsAnthony ChenYicheng FanHagai TaitelbaumHila NogaZhuyun DaiJames WangChen LiangJenny HamerChun-Sung FerngChenel ElkindAviel AtiasPaulina Lee\nContributorsVít ListíkMathias CarlenJan van de KerkhofMarcin PikusKrunoslav ZaherPaul MüllerSasha ZykovaRichard StefanecVitaly GatskoChristoph HirnschallAshwin SethiXingyu Federico XuChetan AhujaBeth TsaiAnca StefanoiuBo FengKeshav DhandhaniaManish KatyalAkshay GuptaAtharva ParulekarDivya PittaJing ZhaoVivaan BhatiaYashodha BhavnaniOmar AlhadlaqXiaolin LiPeter DanenbergDennis TuAlex PineVera FilippovaAbhipso GhoshBen LimonchikBhargava UralaChaitanya Krishna LankaDerik CliveYi SunEdward LiHao WuKevin HongtongsakIanna LiKalind ThakkarKuanysh OmarovKushal MajmundarMichael AlversonMichael KucharskiMohak PatelMudit JainMaksim Zabelin\n68"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nContributorsPaolo PelagattiRohan KohliSaurabh KumarJoseph KimSwetha SankarVineet ShahLakshmi RamachandruniXiangkai ZengBen BariachLaura WeidingerTu VuAlek AndreevAntoine HeKevin HuiSheleem Kashem\nGemini App Program LeadsAmar Subramanya7\nSissie Hsiao\nGemini Program LeadsDemis HassabisKoray Kavukcuoglu\nOverall Gemini App Technical LeadsAdam Sadovsky8\nQuoc LeTrevor Strohman9\nYonghui Wu10\nOverall Gemini Post-Training LeadSlav Petrov\nOverall Gemini Technical Leads (equal con-tribution)Jeffrey DeanOriol Vinyals\nThe roles are defined as below:\n• Lead: Individual(s) responsible for the sub-team throughout the project.• Core Contributor: Individual that had significant impact throughout the project.• Contributor: Individual that had contributions to the project and was partially involved with theeffort.• Program Lead: Responsible for the organizational aspects of the Gemini effort.• Overall Post-Training Lead: Responsible for the technical direction of post-training.• Overall Technical Lead: Responsible for the technical direction of the overall Gemini effort.\nWithin each role, contributions are equal, and are listed in a randomized order. Ordering withineach role does not indicate ordering of the contributions.\nGemini is a cross-Google effort, with members from Google DeepMind (GDM), Google Research(GR), Bard/Assistant, Knowledge and Information (K&I), Core ML, Cloud, Labs, and more.\nWe thank Aakanksha Chowdhery, Dustin Tran, Heng-Tze Cheng, Jack W. Rae, Kate Olszewska,Mariko Iinuma, Peter Humphreys, Shashi Narayan, and Steven Zheng for leading the preparation ofthis report. We also thank our reviewers and colleagues for their valuable discussions and feedbackon the report — Alexandra Belias, Ana Ramalho, Anand Rao, Arielle Bier, Danielle Landress, EleanorTomlinson, Emily Hossellman, Gaby Pearl, Helen King, Hollie Dobson, Jaclyn Konzelmann, Jennifer\n7Lead, Gemini App Engineering8Lead, Gemini App Core Modeling, Eval, Data9Co-Lead, Gemini App Serving10Co-Lead, Gemini Text\n69"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nBeroshi, Joel Moss, Jon Small, Jonathan Fildes, Kathy Meier-Hellstern, Lisa Patel, Oli Gaymond,Rebecca Bland, Reena Jana, Tessa Lueth, and Tom Lue.\nOur work is made possible by the dedication and efforts of numerous teams at Google. We wouldlike to acknowledge the support from Abhi Mohan, Adekunle Bello, Aishwarya Nagarajan, AlaaSaade, Alejandro Lince, Alexander Chen, Alexander Kolbasov, Alexander Schiffhauer, Ameya Shringi,Amin Vahdat, Anda Rabatić, Anthonie Gross, Antoine Yang, Anthony Green, Anton Ruddock, ArtKhurshudov, Artemis Chen, Arthur Argenson, Avinatan Hassidim, Beiye Liu, Benjamin Schroeder,Bin Ni, Brett Daw, Bryan Chiang, Burak Gokturk, Carl Crous, Carrie Grimes Bostock, Charbel Kaed,Charlotte Banks, Che Diaz, Chris Larkin, Christy Lian, Claire Cui, Clare Bycroft, Corentin Tallec,Daniel Herndon, Dave Burke, David Battle, David Engel, Dipannita Shaw, Donghyun Koo, DougRitchie, Dragos Stefanescu, Elissa Wolf, Emre Sargin, Eric Herren, Estella King, Fatema Alkhanaizi,Felix Gimeno, Fernando Pereira, Florent Altché, Gabriel Carvajal, Gaurav Gandhi, George Powell,Goran Pavičić, Harry Richardson, Hassan Wassel, Hongji Li, Idan Szpektor, Igor Ivanisevic, IvanJambrešić, Ivan Jurin, Jade Fowler, James Assiene, Jay Yagnik, Jean-bastien Grill, Jeff Seibert, JennaLaPlante, Jessica Austin, Jianxing Lu, Jim O’Keeffe, Jin Huang, Joe Heyward, Johannes Welbl, JohnJumper, Jonathan Caton, Josh Woodward, Joshua Foster, Kathryn Tunyasuvunakool, Katrina Wong,Kavya Kopparapu, Kelvin Nguyen, Kira Yin, Konstantin Sharlaimov, Kun Li, Lee Hong, Lilly Taylor,Longfei Shen, Luc Mercier, Maciej Mikuła, Mania Abdi, Manuel Sanchez, Maria Ines Aranguren,Mario Carlos Cortes III, Matthew Tait, Matthias Lochbrunner, Mehdi Ghissassi, Micah Mosley, MichaelBendersky, Michael Figurnov, Michael Harris, Michael Mathieu, Michael O’Neill, Michael Vorburger,Mihir Paradkar, Nandita Dukkipati, Nathan Carter, Nathan Watson, Neil Rabinowitz, Nikhil Dandekar,Nishant Ranka, Olcan Sercinoglu, Olivier Lacombe, Ottavia Bertolli, Paul Caron, Pranesh Srinivasan,Praveen Kumar, Rahul Sukthankar, Raia Hadsell, Rajagopal Ananthanarayanan, Roberto Lupi, RosieZou, Sachin Menezes, Sadegh Jazayeri, Sam Cheung, Sameer Bidichandani, Sania Alex, SanjivKumar, Sara Wiltberger, Sarah Fitzgerald, Saz Basu, Sebastian Nowozin, Shannon Hepburn, ShayneCardwell,Srinivasan Venkatachary, Sugato Basu, Sundar Pichai, Sundeep Tirumalareddy, SusannahYoung, Swetha Vijayaraghavan, Tania Bedrax-Weiss, Taylor Applebaum, Teiva Harsanyi, Terry Chen,Tim Blyth, Ting Liu, Tom Cobley, Tomas Izo, Trystan Upstill, Varun Singhai, Vedrana Klarić Trupčević,Victor Cai, Vladimir Pudovkin, Vu Dang, Wenbo Zhao, Wesley Crow, Wesley Szeng, Xiaodan Song,Yazhou Zu, Ye Tian, Yicong Wang, Yixing Wang, Yossi Matias, Yunlong Jiao, Zachary Jessup, ZhenchuanPang, Žiga Avsec, Zimeng Yang, and Zoubin Ghahramani. We’d also like to recognize the AlphaCodeteam, the Borg Scheduling team, the Facilities team, the Gemini Demo Team, the Global Server Ops(GSO) team, the JAX team, the the Legal team, ML SRE team, the ML Supercomputer (MLSC) team,the PartIR team, the Platforms Infrastructure Engineering (PIE) team, and the XLA Compiler team.\nWe thank everyone at Google not explicitly mentioned above, who have shared excitement, givenfeedback on early Gemini models or created interesting demo uses of Gemini, and worked with orsupported the core Gemini team on many aspects of this project.\n70"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n\n10. Appendix\n10.1. Gemini Ultra Model Card\nModel summary\nModel architectureGemini V1.0 is a new family of state-of-the-art language models,containing variants known as Nano, Pro and Ultra (orderedby parameter count) based on a decoder-only Transformerarchitecture (Vaswani et al., 2017a). Models are trained tosupport 32K context length, employing efficient attentionmechanisms such as multi-query attention (Shazeer, 2019b).Gemini is trained jointly across image, audio, video and textdata for the purpose of building a model with both stronggeneralist capabilities across modalities alongside cutting-edgeunderstanding and reasoning performance in each respectivedomain.\nThe post-trained models described in this model cardare Gemini API and Gemini Apps model variants (Section 6)built on top of the Gemini Ultra pre-trained model. During thepost-training process, additional architectural modifications arealso made to support the training of multi-objective rewardmodels for RLHF.\nInput(s)Text (e.g. a question, a prompt, a document(s) to be summa-rized), images, video, audio files.\nOutput(s)Generated text in response to the input (e.g. an answer tothe question, a summary of multiple documents, comparingdocuments/videos).\nUsage\nApplicationGemini is designed for accelerating research on languagemodels, for use as a building block in features within Googleproducts, and as a building block for select applications such asGemini App and Search Generative Experience.\nServices and products built on top of Gemini Ultra arealso being made available to external developers via GoogleCloud Vertex API and Google Labs, with additional process andtechnical safeguards related to safety policies.\nKnown CaveatsGemini should not be made available as part of a general-purposeservice or product, or used within a specific downstream appli-cation without a prior assessment and mitigation of the safetyand fairness concerns specific to the downstream use.\n71"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nImplementation Frameworks\nHardware & SoftwareHardware: Training was conducted on TPUv4 and TPUv5e(Jouppi et al., 2020, 2023).\nSoftware:JAX (Bradbury et al., 2018),ML Pathways(Dean, 2021).\nJAX allows researchers to leverage the latest generationof hardware, including TPUs, for faster and more efficienttraining of large models.\nML Pathways is infrastructure software to support Google’sefforts to build artificially intelligent systems capable ofgeneralizing across multiple tasks. This is specially suitable forfoundation models, including large language models like theGemini V1.0 models.\nTogether, JAX and ML Pathways are used as described inSection 3.The ’single controller’ programming model ofJAX and ML Pathways allows a single Python process toorchestrate the entire training run, dramatically simplifying thedevelopment workflow.\nCompute RequirementsNot reported.\nModel Characteristics\nModel initializationInitial pretraining used random initialization. Post-training wasinitialized from checkpoints obtained at the later stages of pre-training. These checkpoints were fine-tuned using supervisedfine-tuning, and subsequently used to initialize reward modeltraining and RLHF.\nModel StatusThis is a static model trained on an offline dataset.\nModel StatsNot reported.\nData overview\nTraining DatasetGemini models are trained on a dataset that is both multimodaland multilingual. Our pre-training dataset uses data from webdocuments, books, and code, and includes image, audio, andvideo data.\nRefer to Section 4 (Pre-Training Dataset) for further de-tails.\n72"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nEvaluation DatasetWe compare pre- and post-trained Gemini Ultra models to asuite of external LLMs and our previous best model PaLM 2across a series of text-based academic benchmarks coveringreasoning, reading comprehension, STEM, and coding.\nWe also evaluate Gemini models on four different mul-timodal capabilities:high-level object recognition usingcaptioning or question-answering tasks such as VQAv2; fine-grained transcription using tasks such as TextVQA and DocVQArequiring the model to recognize low-level details; chartunderstanding requiring spatial understanding of input layoutusing ChartQA and InfographicVQA tasks; and multimodalreasoning using tasks such as Ai2D, MathVista and MMMU.\nRefer to Section 5 (Evaluation) for further details.\nPost-training DatasetFor post-training, we first collect a diverse set of prompts thatare representative of real-world use cases. We then collectdemonstration data of what the model’s output should be fora given prompt for supervised fine-tuning. We further collectdifferent possible responses to a given prompt, and collectfeedback data over these to train reward models.\nRefer to Section 6.3 (Post-Training Methods and Data)for further details.\nEvaluation Results\nBenchmark InformationSee Section 5 (Evaluation).\nEvaluation ResultsSee Section 5 (Evaluation) and Section 6.4 (Post-Training Hu-man Evaluation).\nModel Usage & Limitations\nSensitive UseFor an analysis of risks and sensitive uses associated with theGemini models, see Section 7.1 (Impact Assessment).\nKnown LimitationsGemini models can exhibit limitations outlined in Section 7.1(Impact Assessment). Gemini models should not be used fordownstream applications without further analysis of potentialharm in the proposed downstream application.\nEthical Considerations &RisksA reflection on the potential risks and impacts of the Gemini V1.0models can be found in Section 7 (Responsible Deployment).For evaluation details for a range of risks, see Section 7.4 (SafetyEvaluations).\n10.2. Chain-of-Thought Comparisons on MMLU benchmark\nWe contrast several chain-of-thought approaches on MMLU and discuss their results in this section. Weproposed a new approach where model produces k chain-of-thought samples, selects the majority voteif the model is confident above a threshold, and otherwise defers to the greedy sample choice. The\n73"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nthresholds are optimized for each model based on their validation split performance. The proposedapproach is referred to as uncertainty-routed chain-of-thought. The intuition behind this approachis that chain-of-thought samples might degrade performance compared to the maximum-likelihooddecision when the model is demonstrably inconsistent. We compare the gains from the proposedapproach on both Gemini Ultra and GPT-4 in Figure 9. We find that Gemini Ultra benefits more fromthis approach compared to using only chain-of-thought samples. GPT-4’s performance improves from84.2% with greedy sampling to 87.3% with uncertainty-routed chain-of-thought approach with 32samples, but it already achieves these gains from using 32 chain-of-thought samples. In contrast,Gemini Ultra improves its performance significantly from 84.0% with greedy sampling to 90.0% withuncertainty-routed chain-of-thought approach with 32 samples while it marginally improves to 85.0%with the use of 32 chain-of-thought samples only.\n87.2987.2984.2184.99\n90.0483.96\nScore EvalChain-of-Thought@32Chain-of-Thought@32\n(Uncertainty-Routed)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nGPT-4 (gpt-4-0613)Gemini Ultra\n\nMMLU accuracy (test split)\n\nFigure 9 | Chain-of-Thought with uncertainty routing on MMLU.\n74"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.3. Capabilities and Benchmarking Tasks\nWe use more than 50 benchmarks as a holistic harness to evaluate the Gemini models across text,image, audio and video. We provide a detailed list of benchmarking tasks for six different capabilities intext understanding and generation: factuality, long context, math/science, reasoning, summarization,and multilinguality. We also enumerate the benchmarks used for image understanding, videounderstanding, and audio understanding tasks.\n• Factuality: We use 5 benchmarks: BoolQ (Clark et al., 2019), NaturalQuestions-Closed(Kwiatkowski et al., 2019a), NaturalQuestions-Retrieved (Kwiatkowski et al., 2019a), Real-timeQA (Kasai et al., 2022b), TydiQA-noContext and TydiQA-goldP (Clark et al., 2020).• Long Context: We use 6 benchmarks: NarrativeQA (Kočiský et al., 2018), Scrolls-Qasper,Scrolls-Quality (Shaham et al., 2022), XLsum (En), XLSum (non-English languages) (Hasanet al., 2021), and one other internal benchmark.• Math/Science: We use 8 benchmarks: GSM8k (with CoT) (Cobbe et al., 2021), Hendryck’sMATH pass@1 (Hendrycks et al., 2021b), MMLU (Hendrycks et al., 2021a), Math-StackExchange,Math-AMC 2022-2023 problems, and three other internal benchmarks.• Reasoning: We use 7 benchmarks: BigBench Hard (with CoT) (Srivastava et al., 2022; Suzgunet al., 2022), CLRS (Veličković et al., 2022), Proof Writer (Tafjord et al., 2020), Reasoning-Fermiproblems (Kalyan et al., 2021), Lambada (Paperno et al., 2016), HellaSwag (Zellers et al.,2019), DROP (Dua et al., 2019).• Summarization: We use 5 benchmarks: XL Sum (English), XL Sum (non-English languages)(Hasan et al., 2021), WikiLingua (non-English languages), WikiLingua (English) (Ladhak et al.,\n2020), XSum (Narayan et al., 2018).• Multilinguality: We use 10 benchmarks: XLSum (Non-English languages) (Hasan et al., 2021),WMT22 (Kocmi et al., 2022), WMT23 (Tom et al., 2023), FRMT (Riley et al., 2023), WikiLingua(Non-English languages) (Ladhak et al., 2020), TydiQA (no context), TydiQA (GoldP) (Clarket al., 2020), MGSM (Shi et al., 2023), translated MMLU (Hendrycks et al., 2021a), NTREX(Federmann et al., 2022), FLORES-200 (Team et al., 2022).• Image and Video: We use 9 benchmarks for image understanding: MMMU (Yue et al., 2023),TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022),InfographicVQA (Mathew et al., 2022), MathVista (Lu et al., 2023), AI2D (Kembhavi et al.,2016), VQAv2 (Goyal et al., 2017), XM3600 (Thapliyal et al., 2022) for multi-lingual imageunderstanding, and 6 benchmarks for video understanding: VATEX (Wang et al., 2019) forcaptioning in two different languages, YouCook2 (Zhou et al., 2018), NextQA (Xiao et al.,2021), ActivityNet-QA (Yu et al., 2019), and Perception Test MCQA (Pătrăucean et al., 2023).• Audio: We use 5 benchmarks including automatic speech recognition (ASR) tasks such asFLEURS (Conneau et al., 2023), VoxPopuli (Wang et al., 2021), Multi-lingual Librispeech (Pratapet al., 2020), and automatic speech translation task such as CoVoST 2 (Wang et al., 2020).\n75"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4. Qualitative Examples\nThis section shows sample qualitative examples from prompting the Gemini Ultra model. Someillustrative examples of multimodal reasoning for image understanding tasks over charts, naturalimages and memes are shown in Figures 10, 11, 13, 15, 16, and 17. Figure 12 shows an example ofimage generation capabilities of Gemini Ultra where the user generates an interleaved sequence ofimage and text to design a blog post. Beyond English, Figure 18 shows model’s capability to understandimages in a multilingual setting. Gemini models also show strong performance on multimodal imageunderstanding and reasoning in mathematics, as shown in Figures 14, 20 and 21. Figure 22 is anexample of complex multimodal reasoning demonstrating how the model composes complex imageunderstanding, code generation, and instruction following capabilities for a given user task. In Figure19, we see another example of the model being able to generate working code and follow complexuser instructions. Finally, Figure 23 shows an example of Gemini Ultra’s capability of understandingvideo by reasoning over temporally connected set of frames.\n76"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.1. Chart understanding and reasoning over data\nPrompt\nSpot a data point that stands out in these charts and what that implicates. Then produce adetailed markdown table for all the data shown.\nModel Response (rendered Markdown)\nThe data point that stands out is the United States’ landfilled plastic waste, which is 73%. Thisis significantly higher than the other countries and regions shown. This implies that the UnitedStates is not doing as much to recycle or incinerate its plastic waste as other countries andregions.\nCountry/RegionLandfilled (%)Mismanaged (%)Incinerated (%)Recycled (%)\nWorld4922199\nUnited States734194\nEurope4463812\nAsia (excl. China andIndia)\n3934198\nFigure 10 | Solving a problem requiring multimodal chart understanding.The model has to read thetext, understand the connections between different data points and reason over them to recommendan interesting point and follow the instructions to generate a markdown table (shown correctlyrendered).Source: Our World In Data (Ritchie et al., 2023).\n77"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.2. Multimodal question answering\nPrompt\nDo you know what it this plant? How do I best take care of it?\nModel Response\nThis is a Persian shield plant. It is a tropical plant that is native to Southeast Asia. It is apopular houseplant because it is easy to care for and has beautiful purple leaves.\nPersian shield plants prefer bright, indirect light. They can tolerate some direct sunlight, buttoo much direct sunlight can scorch their leaves. They also prefer warm temperatures andhigh humidity.\nPersian shield plants need to be watered regularly, but they should not be allowed to sit inwater. The soil should be kept moist but not soggy. They also benefit from being misted regularly.\nPersian shield plants can be fertilized every two weeks during the growing season. A balancedfertilizer is a good choice.\nPersian shield plants are susceptible to pests, such as mealybugs and spider mites. These pestscan be controlled with insecticidal soap or neem oil.\nPersian shield plants can be propagated by stem cuttings. The cuttings should be taken in thespring or summer and should be rooted in water or a moist potting mix.\nFigure 11 | Answering a multimodal information-seeking query. The model is able to recognize thespecific plant shown in the image and provide information about it. The model shows robustness totypos as it is able to understand the user question despite them.Source: photo taken by an author from the Gemini team.\n78"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p78_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.2. Multimodal question answering\nPrompt\nDo you know what it this plant? How do I best take care of it?\nModel Response\nThis is a Persian shield plant. It is a tropical plant that is native to Southeast Asia. It is a\npopular houseplant because it is easy to care for and has beautiful purple leaves.\nPersian shield plants prefer bright, indirect light. They can tolerate some direct sunlight, but\ntoo much direct sunlight can scorch their leaves. They also prefer warm temperatures and\nhigh humidity.\nPersian shield plants need to be watered re"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.3. Interleaved image and text generation\nPrompt\nPlease create a blog post about a trip to New York, where a dog and his owner had lots of fun.Include a few pictures of the dog posing happily at different landmarks.\nModel Response\nHi, I’m Cooper. I’m a 2 year old Goldendoodle who loves to travel. Last month, my family andI went to New York City. It was my first time in the Big Apple, and I had a blast!\nWe stayed in a hotel near Times Square. It was so much fun to see all the bright lights andpeople. We went to see a Broadway show, and I even got to meet some of the actors.\nOne of my favorite parts of New York City was Central Park. It was so big and beautiful. Iloved running around and playing fetch. We also went to the Statue of Liberty and the EmpireState Building. I was so excited to see all the famous landmarks.\nI had so much fun in New York City. It’s definitely one of my favorite places to travel. I can’twait to go back someday!\nFigure 12 | Generating interleaved text and images. The model is able to follow the instructions ofgenerating a blog post with images closely related to the text and with dog images showing highlevels of consistency across all images.\n79"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p79_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.3. Interleaved image and text generation\nPrompt\nPlease create a blog post about a trip to New York, where a dog and his owner had lots of fun.\nInclude a few pictures of the dog posing happily at different landmarks.\nModel Response\nHi, I’m Cooper. I’m a 2 year old Goldendoodle who loves to travel. Last month, my family and\nI went to New York City. It was my first time in the Big Apple, and I had a blast!\nWe stayed in a hotel near Times Square. It was so much fun to see all the bright lights and\npeople. We went to see a Broadway show, and"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p79_img2.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.3. Interleaved image and text generation\nPrompt\nPlease create a blog post about a trip to New York, where a dog and his owner had lots of fun.\nInclude a few pictures of the dog posing happily at different landmarks.\nModel Response\nHi, I’m Cooper. I’m a 2 year old Goldendoodle who loves to travel. Last month, my family and\nI went to New York City. It was my first time in the Big Apple, and I had a blast!\nWe stayed in a hotel near Times Square. It was so much fun to see all the bright lights and\npeople. We went to see a Broadway show, and"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p79_img3.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.3. Interleaved image and text generation\nPrompt\nPlease create a blog post about a trip to New York, where a dog and his owner had lots of fun.\nInclude a few pictures of the dog posing happily at different landmarks.\nModel Response\nHi, I’m Cooper. I’m a 2 year old Goldendoodle who loves to travel. Last month, my family and\nI went to New York City. It was my first time in the Big Apple, and I had a blast!\nWe stayed in a hotel near Times Square. It was so much fun to see all the bright lights and\npeople. We went to see a Broadway show, and"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.4. Image understanding and reasoning\nPrompt\nLook at this sequence of three shapes. What shape should come as the fourth shape? Explainyour reasoning with detailed descriptions of the first shapes.\nModel Response\nThe fourth shape should be a hexagon. The first shape is a triangle, the second shape is asquare, and the third shape is a pentagon. The number of sides in each shape is increasingby one\nFigure 13 | Solving a multimodal reasoning problem.The model is able to recognize shapes in theimage, understand their properties and reason about the relationship between them to predict thenext object.Source: photo taken by an author from the Gemini team.\n80"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p80_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.4. Image understanding and reasoning\nPrompt\nLook at this sequence of three shapes. What shape should come as the fourth shape? Explain\nyour reasoning with detailed descriptions of the first shapes.\nModel Response\nThe fourth shape should be a hexagon. The first shape is a triangle, the second shape is a\nsquare, and the third shape is a pentagon. The number of sides in each shape is increasing\nby one\nFigure 13 | Solving a multimodal reasoning problem.The model is able to recognize shapes in the\nimage, understand their properties and reaso"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.5. Geometrical reasoning\nPrompt\nFind the height of the parallelogram given its area with 100 square units.\nModel Response\nThe area of the parallelogram is equal to the product of the base and the height. Hence100 = (𝑥+ 15)𝑥. We get 𝑥2+ 15𝑥−100 = 0. The roots of this equation are 𝑥1 = 5 and𝑥2 = −20. The second root is negative and is not acceptable. Final Answer: The final answeris 5. I hope it is correct.\nFigure 14 | Solving a geometrical reasoning task. The model shows good understanding of the taskand is able to provide meaningful reasoning steps despite slightly unclear instructions.Source: Lu et al. (2021).\n10.4.6. Information seeking about objects\nPrompt\nFind a connection between these. Hint: think about historical events.\nModel Response\nThe Moon is the only celestial body in which humans have played golf. In 1971, the Apollo14 crew hit two golf balls on the lunar surface.\nFigure 15 | Solving a puzzle using multimodal inputs. The model recognizes the objects in the imagesand identifies a commonality that connects the two objects.Source: photo taken by an author from the Gemini team.\n81"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p81_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.5. Geometrical reasoning\nPrompt\nFind the height of the parallelogram given its area with 100 square units.\nModel Response\nThe area of the parallelogram is equal to the product of the base and the height. Hence\n100 = (𝑥+ 15)𝑥. We get 𝑥2 + 15𝑥−100 = 0. The roots of this equation are 𝑥1 = 5 and\n𝑥2 = −20. The second root is negative and is not acceptable. Final Answer: The final answer\nis 5. I hope it is correct.\nFigure 14 | Solving a geometrical reasoning task. The model shows good understanding of the task\nand is able to provide meaningfu"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p81_img2.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.5. Geometrical reasoning\nPrompt\nFind the height of the parallelogram given its area with 100 square units.\nModel Response\nThe area of the parallelogram is equal to the product of the base and the height. Hence\n100 = (𝑥+ 15)𝑥. We get 𝑥2 + 15𝑥−100 = 0. The roots of this equation are 𝑥1 = 5 and\n𝑥2 = −20. The second root is negative and is not acceptable. Final Answer: The final answer\nis 5. I hope it is correct.\nFigure 14 | Solving a geometrical reasoning task. The model shows good understanding of the task\nand is able to provide meaningfu"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p81_img3.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.5. Geometrical reasoning\nPrompt\nFind the height of the parallelogram given its area with 100 square units.\nModel Response\nThe area of the parallelogram is equal to the product of the base and the height. Hence\n100 = (𝑥+ 15)𝑥. We get 𝑥2 + 15𝑥−100 = 0. The roots of this equation are 𝑥1 = 5 and\n𝑥2 = −20. The second root is negative and is not acceptable. Final Answer: The final answer\nis 5. I hope it is correct.\nFigure 14 | Solving a geometrical reasoning task. The model shows good understanding of the task\nand is able to provide meaningfu"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.7. Multimodal reasoning based on visual cues\nPrompt\nDo you know the precise location where this image was taken?\nModel Response\nThis image was taken in New York City. The Empire State Building is visible in the background.The street in the foreground is 8th Avenue. The cross street is West 34th Street.\nFigure 16 | Identifying the objects in the image (the Empire State Building) and recognizing whatthose are even with small levels of visual distortion in the image. Based on the image, the model isalso able to correctly identify the precise location of the person taking the photo.Source: photo taken by an author from the Gemini team.\n82"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p82_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.7. Multimodal reasoning based on visual cues\nPrompt\nDo you know the precise location where this image was taken?\nModel Response\nThis image was taken in New York City. The Empire State Building is visible in the background.\nThe street in the foreground is 8th Avenue. The cross street is West 34th Street.\nFigure 16 | Identifying the objects in the image (the Empire State Building) and recognizing what\nthose are even with small levels of visual distortion in the image. Based on the image, the model is\nalso able to correctly identify the pr"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.8. Multimodal humor understanding\nPrompt\nCan you explain this meme?\nModel Response\nThe meme is about a person who is playing a game at 300 FPS, but their monitor is only75Hz. The person is trying to catch the game, but it is too fast for their monitor to display.The meme is funny because it is relatable to anyone who has ever played a game on a monitorthat is not fast enough.\nFigure 17 | Explanation of humor in a meme. The model is showing the ability to not only describewhat is happening in the image but also what it means even though the cultural context is notmentioned explicitly in the image or the prompt.Source: Hwang and Shwartz (2023).\n83"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p83_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.4.8. Multimodal humor understanding\nPrompt\nCan you explain this meme?\nModel Response\nThe meme is about a person who is playing a game at 300 FPS, but their monitor is only\n75Hz. The person is trying to catch the game, but it is too fast for their monitor to display.\nThe meme is funny because it is relatable to anyone who has ever played a game on a monitor\nthat is not fast enough.\nFigure 17 | Explanation of humor in a meme. The model is showing the ability to not only describe\nwhat is happening in the image but also what it means even tho"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.5. Commonsense reasoning in a multilingual setting\nPrompt\nExplain what i should call my grandparents on each side of my family based on this chart\nModel Response\nOn your father’s side, you would call your grandparents Yéye and Nˇainai. On your mother’sside, you would call your grandparents Wàig¯ong and Wàipó.\nFigure 18 | Common-sense reasoning in images. The model is able to understand the relationshipsrepresented in the graphs and reason about them in a multilingual setting.Source: image created by an author from the Gemini team.\n84"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p84_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.5. Commonsense reasoning in a multilingual setting\nPrompt\nExplain what i should call my grandparents on each side of my family based on this chart\nModel Response\nOn your father’s side, you would call your grandparents Yéye and Nˇainai. On your mother’s\nside, you would call your grandparents Wàig¯ong and Wàipó.\nFigure 18 | Common-sense reasoning in images. The model is able to understand the relationships\nrepresented in the graphs and reason about them in a multilingual setting.\nSource: image created by an author from the Gemini team.\n84"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.5.1. Reasoning and code generation\nPrompt\nCreate a web app called \"Opossum Search\":\n1. Every time you make a search query, it should redirect you to a google search with the samequery, but the word opossum before it.2. It should be visually similar to Google search,3. Instead of the google logo, it should have a picture of an opossum from the internet.4. It should be a single html file, no separate js or css files.5. It should say \"Powered by google search\" in the footer.\nModel Response (rendered code)\nHere is the code for Opossum Search:\n1<!DOCTYPE html>2<html>3<head>4<title>Opossum Search</title>5<style>6body {7font−family: Arial, sans−serif;8margin: 0;9padding: 0;10}1112header {13background−color: #333;14color: #fff;15text−align: center;16padding: 1rem 0;17}1819.search−bar {20display: flex;21justify−content: center;22align−items: center;23height: 50px;24margin−top: 1rem;25}2627.search−input {28width: 50%;29padding: 0.5rem;30border: 1px solid #ccc;31border−radius: 5px;32}3334.search−button {35margin−left: 1rem;36padding: 0.5rem 1rem;37border: 1px solid #ccc;38border−radius: 5px;39cursor: pointer;40}4142footer {43background−color: #333;44color: #fff;45text−align: center;46padding: 1rem 0;47margin−top: 1rem;48}49</style>50</head>51<body>52<header>53<h1>Opossum Search</h1>54<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Didelphis_virginiana.jpg/551200px−Didelphis_virginiana.jpg\" alt=\"Opossum\" width=\"100\">56</header>5758<div class=\"search−bar\">59<input type=\"text\" class=\"search−input\" placeholder=\"Search the web\">60<button class=\"search−button\">Search</button>61</div>\n85"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n62<footer>63Powered by Google Search64</footer>6566<script>67const searchInput = document.querySelector('.search−input');68const searchButton = document.querySelector('.search−button');6970searchButton.addEventListener('click',() => {71const query = searchInput.value;72if (query) {73window.location.href = `https://www.google.com/search?q=opossum+${query}`;74}75});76</script>77</body>\nThis code creates a simple web app that looks similar to Google Search, but with an opossumlogo. When you enter a search query and click the \"Search\" button, it will redirect you to aGoogle search with the word \"opossum\" added to the beginning of your query. The app ispowered by Google Search, as indicated in the footer.\nRendered Website\nFigure 19 | Writing code for a website based on user request. The model follows the instructions andrequirements defined by the user and converts them to functioning code.\n86"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p86_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n62\n<footer>\n63\nPowered by Google Search\n64\n</footer>\n65\n66\n<script>\n67\nconst searchInput = document.querySelector('.search−input');\n68\nconst searchButton = document.querySelector('.search−button');\n69\n70\nsearchButton.addEventListener('click',\n() => {\n71\nconst query = searchInput.value;\n72\nif (query) {\n73\nwindow.location.href = `https://www.google.com/search?q=opossum+${query}`;\n74\n}\n75\n});\n76\n</script>\n77\n</body>\nThis code creates a simple web app that looks similar to Google Search, but with an opossum\nlogo. When you enter a search query an"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.5.2. Mathematics: Calculus\nPrompt\nFind the derivative of sinh 𝑥+ cosh 𝑦= 𝑥+ 𝑦.\nModel Response (rendered latex)\nWe have that𝑑𝑑𝑥sinh 𝑥= cosh 𝑥and𝑑𝑑𝑥cosh 𝑦= sinh 𝑦𝑑𝑦\n𝑑𝑥. Therefore,\ncosh 𝑥+ sinh 𝑦𝑑𝑦\n𝑑𝑥= 1 + 𝑑𝑦\n𝑑𝑥.\nSolving for𝑑𝑦\n𝑑𝑥, we find that\n𝑑𝑦𝑑𝑥= 1 −cosh 𝑥\nsinh 𝑦−1.\nFinal Answer: The final answer is1−cosh 𝑥\nsinh 𝑦−1.\nFigure 20 | Solving a calculus problem. The model is able to get a solution to a calculus problem withstep-by-step explanation and correctly defined LaTeX equations.Source: question is provided by Macmillan Learning.\n87"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6. Multi-step reasoning and mathematics\nPrompt\nIn the year 2018 factory A produced goods worth 500 million forints, while factory B producedgoods worth 400 million forints. According to their long-term plans, factory A will increaseproduction by 5% each year, while factory B will increase production by 6% per year.\nThe above plans were published on an economy website. A debate started in the commentsection of the article: one user claimed that the difference between the volumes of productionbetween the two factories would get smaller and smaller in the next couple of years.\nCalculate the missing values of the table below and show that the above claim is not true.\nHeader: | 2018 | 2019 | 2020 | 2021 |Production volume of factory A (mln) | 500 | | | |Production volume of factory B (mln) | 400 | | | |\nModel Response (rendered Markdown)\nYearProduction volume offactory A (mln)\nProduction volume offactory B (mln)\nDifference(mln)201850040010020195254241012020551.25449.44101.812021578.81476.41102.4\nThe production volume of factory A in 2019 is 500 * 1.05 = 525 million forints.The production volume of factory B in 2019 is 400 * 1.06 = 424 million forints.The difference in production volume in 2019 is 525 - 424 = 101 million forints.\nThe production volume of factory A in 2020 is 525 * 1.05 = 551.25 million forints.The production volume of factory B in 2020 is 424 * 1.06 = 449.44 million forints.The difference in production volume in 2020 is 551.25 - 449.44 = 101.81 million forints.\nThe production volume of factory A in 2021 is 551.25 * 1.05 = 578.81 million forints.The production volume of factory B in 2021 is 449.44 * 1.06 = 476.41 million forints.The difference in production volume in 2021 is 578.81 - 476.41 = 102.4 million forints.\nAs you can see, the difference in production volume between the two factories is actuallyincreasing, not decreasing. Therefore, the claim that the difference between the volumes ofproduction between the two factories would get smaller and smaller in the next couple of yearsis not true.\nFigure 21 | Solving a multi-step math problem. The model is able to understand the task and generatea markdown table with correctly calculated values. It also explicitly follows the instructions to showwhere the numbers come from and answer the question given in the task.Source: Oktatási Hivatal (2023, p. 20)\n88"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.1. Complex image understanding, code generation, and instruction following\nPrompt\nI want you to take the function depicted in the top left subplot, multiply it by 1000, and thenadd it to the function depicted in the bottom left subplot. Generate matplotlib code for thesingle resulting plot.\nModel Response (rendered code)\n1import numpy as np2import matplotlib.pyplot as plt34x = np.linspace(0, 10, 1000)5y1 = np.sin(x)6y2 = np.exp(x)78plt.plot(x,1000∗y1 + y2)9plt.show()\nRendered Graph\nFigure 22 | Multimodal reasoning capabilities applied to code generation. Gemini Ultra needs toperform inverse graphics task to infer the code that would have generated the plots, perform additionalmathematical transformations, and generate relevant code.Source: figure generated by an author from the Gemini team.\n89"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p89_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.1. Complex image understanding, code generation, and instruction following\nPrompt\nI want you to take the function depicted in the top left subplot, multiply it by 1000, and then\nadd it to the function depicted in the bottom left subplot. Generate matplotlib code for the\nsingle resulting plot.\nModel Response (rendered code)\n1\nimport numpy as np\n2\nimport matplotlib.pyplot as plt\n3\n4\nx = np.linspace(0, 10, 1000)\n5\ny1 = np.sin(x)\n6\ny2 = np.exp(x)\n7\n8\nplt.plot(x,\n1000∗y1 + y2)\n9\nplt.show()\nRendered Graph\nFigure 22 | Multimodal reasoning capa"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p89_img2.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.1. Complex image understanding, code generation, and instruction following\nPrompt\nI want you to take the function depicted in the top left subplot, multiply it by 1000, and then\nadd it to the function depicted in the bottom left subplot. Generate matplotlib code for the\nsingle resulting plot.\nModel Response (rendered code)\n1\nimport numpy as np\n2\nimport matplotlib.pyplot as plt\n3\n4\nx = np.linspace(0, 10, 1000)\n5\ny1 = np.sin(x)\n6\ny2 = np.exp(x)\n7\n8\nplt.plot(x,\n1000∗y1 + y2)\n9\nplt.show()\nRendered Graph\nFigure 22 | Multimodal reasoning capa"}
{"metadata":{"type":"text","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.2. Video understanding and reasoning\nPrompt (video)\nHow could this person improve their technique?\nModel Response\nThis soccer player needs to work on their ball-striking mechanics. Their foot is not makinggood contact with the ball, which is causing the ball to fly up and away from the goal. Theyalso need to work on their body positioning. They should be leaning into the shot with theirnon-kicking leg in front of their kicking leg, and they should be following through with theirkicking leg.\nFigure 23 | Video understanding and reasoning over the situation presented in the video. Here, weprovide a video as input to the model together with a text prompt (images are provided here onlyfor visualization purposes). The model is able to analyze what happened in the video and providerecommendations on how the actions in the video could have been better.Video source: \"Football/Soccer Penalty Miss\"https://www.youtube.com/watch?v=VmWxjmJ3mvs\n90"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p90_img1.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.2. Video understanding and reasoning\nPrompt (video)\nHow could this person improve their technique?\nModel Response\nThis soccer player needs to work on their ball-striking mechanics. Their foot is not making\ngood contact with the ball, which is causing the ball to fly up and away from the goal. They\nalso need to work on their body positioning. They should be leaning into the shot with their\nnon-kicking leg in front of their kicking leg, and they should be following through with their\nkicking leg.\nFigure 23 | Video understanding and reason"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p90_img2.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.2. Video understanding and reasoning\nPrompt (video)\nHow could this person improve their technique?\nModel Response\nThis soccer player needs to work on their ball-striking mechanics. Their foot is not making\ngood contact with the ball, which is causing the ball to fly up and away from the goal. They\nalso need to work on their body positioning. They should be leaning into the shot with their\nnon-kicking leg in front of their kicking leg, and they should be following through with their\nkicking leg.\nFigure 23 | Video understanding and reason"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p90_img3.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.2. Video understanding and reasoning\nPrompt (video)\nHow could this person improve their technique?\nModel Response\nThis soccer player needs to work on their ball-striking mechanics. Their foot is not making\ngood contact with the ball, which is causing the ball to fly up and away from the goal. They\nalso need to work on their body positioning. They should be leaning into the shot with their\nnon-kicking leg in front of their kicking leg, and they should be following through with their\nkicking leg.\nFigure 23 | Video understanding and reason"}
{"metadata":{"type":"image","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf","image_path":"D:\\PHONGSWORKSPACE\\WORK\\Rag_chatbot_personal\\images\\2312.11805v5_p90_img4.png"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6.2. Video understanding and reasoning\nPrompt (video)\nHow could this person improve their technique?\nModel Response\nThis soccer player needs to work on their ball-striking mechanics. Their foot is not making\ngood contact with the ball, which is causing the ball to fly up and away from the goal. They\nalso need to work on their body positioning. They should be leaning into the shot with their\nnon-kicking leg in front of their kicking leg, and they should be following through with their\nkicking leg.\nFigure 23 | Video understanding and reason"}
{"metadata":{"type":"table","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\nthresholds are optimized for each model based on their validation split performance. The proposed\napproach is referred to as uncertainty-routed chain-of-thought. The intuition behind this approach\nis that chain-of-thought samples might degrade per\n\n| GPT-4 (gpt-4-0613) Gemini Ultra\n90\n90.04\n87.29 87.29\n80 84.21 83.96 84.99\n)\nplit\n70\ns\nst\ne 60\n(t\ny\nc 50\na\nr\nu\nc c 40\na\nU\nL 30\nM\nM\n20\n10\n0\nScore Eval Chain-of-Thought@32 Chain-of-Thought@32\n(Uncertainty-Routed) |  |  |  | GPT-4 (gpt-4-0613) Gemini Ultra |  |  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  | 87.29 87.29\n84.21 83.96 84.99 |  |  |  |  |  |  |  |  | 90.04 |  |\n|  |  |  |  |  |  | 87.29 |  |  | 87.29 |  |  |\n|  |  | 84.21 | 83.96 |  |  |  | 84.99 |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |\n\nse of 32 chain-of-thought samples only.\n87.29\n87.29\n84.21\n84.99\n90.04\n83.96\nScore Eval\nChain-of-Thought@32\nChain-of-Thought@32\n(Uncertainty-Routed)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nGPT-4 (gpt-4-0613)\nGemini Ultra\nMMLU accuracy (test split)\nFigure 9 | Chain-of-Thought with uncertainty routing on MMLU.\n74"}
{"metadata":{"type":"table","source":"https://arxiv.org/pdf/2312.11805","filename":"2312.11805v5.pdf","mime":"application/pdf"},"content":"Gemini: A Family of Highly Capable Multimodal Models\n10.6. Multi-step reasoning and mathematics\nPrompt\nIn the year 2018 factory A produced goods worth 500 million forints, while factory B produced\ngoods worth 400 million forints. According to their long-term plans, factory A will increase\nproduction\n\n| Year | Production volume of\nfactory A (mln) | Production volume of\nfactory B (mln) | Difference\n(mln) |\n| --- | --- | --- | --- |\n| 2018 | 500 | 400 | 100 |\n| 2019 | 525 | 424 | 101 |\n| 2020 | 551.25 | 449.44 | 101.81 |\n| 2021 | 578.81 | 476.41 | 102.4 |\n\ning a multi-step math problem. The model is able to understand the task and generate\na markdown table with correctly calculated values. It also explicitly follows the instructions to show\nwhere the numbers come from and answer the question given in the task.\nSource: Oktatási Hivatal (2023, p. 20)\n88"}
